<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Language Modelling with Recurrent Neural Networks</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style>
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
<!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"> -->
<link rel="stylesheet" type="text/css" href="http://projects.illc.uva.nl/LaCo/clclab/assets/styles.css">
<link rel="stylesheet" href="http://projects.illc.uva.nl/LaCo/clclab/assets/iconic/css/open-iconic-bootstrap.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/contrib/auto-render.min.js"></script><script>document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body);
  });</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
<style>
  body {
    counter-reset: question-counter assignment-counter exercise-counter bonus-counter;
  }

  .exercise, 
  .question,
  .assignment,
  .bonus {
    border-width: 1px;
    border-style: solid;
    padding: .5rem 1rem;
    border-radius: 2px;
    margin: 1rem 0em;
  }

  .question {
    background-color: rgba(50, 150, 150, 0.2);
    border-color: rgba(50, 150, 150, 0.6);
  }

  .exercise {
    background-color: rgba(0, 0, 0, 0.05);
    border-color: rgba(0, 0, 0, 0.2);
  }

  .assignment {
    background-color: rgba(200, 50, 50, 0.2);
    border-color: rgba(200, 50, 50, 0.6);
  }

  .bonus {
    background-color: rgba(250, 150, 50, 0.2);
    border-color: rgba(250, 150, 50, 0.6);
  }

  .exercise:before,
  .question:before,
  .assignment:before,
  .bonus:before {
    display: block;
    font-size: 1rem;
    font-weight: bold;
    margin-bottom: 1em;
  }

  .question:before {
    content: "Question " counter(question-counter);
    counter-increment: question-counter;
  }

  .exercise:before {
    content: "Exercise " counter(exercise-counter);
    counter-increment: exercise-counter;
  }

  .assignment:before {
    content: "Assignment " counter(assignment-counter);
    counter-increment: assignment-counter;
  }

  .bonus:before {
    content: "Bonus " counter(bonus-counter);
    counter-increment: bonus-counter;
	}

	#TOC a {
		color: #999;
	}

	#TOC ul {
		list-style: none;
	}

	#TOC > ul {
		padding: 0;
	}

	#TOC > ul > li {
		padding-bottom: 1em;
	}

	#TOC > ul ul{
		padding-left: 1rem;
	}

	#TOC .toc-section-number {
		display:none;
	}

	article h1 {
		margin-top: 5rem;
		margin-bottom: 1rem;
		line-height: 1em;
	}

	article h2 {
		font-size: 1.5rem;
		margin-top: 2.5rem;
	}

	li p {
		margin: 0;
	}

</style>
</head>
<body>
  <article class="blog">
    <div class="lab-intro">
		<div class="container">
			<header class="offset-lg-3 pt-5 lead my-5">
        <p>
          <span class="badge badge-pill badge-light">Computer lab</span>
        </p>
				<h1 class="display-1">Language Modelling with Recurrent Neural Networks</h1>
        
				<p class="author mt-5">
                    </p>
			</header>
			<div class="clc-pattern bg-div dark">
				<!--?xml version="1.0" encoding="utf-8"?-->

<svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="2000px" height="2000px" viewBox="20 10 2000 2000" xml:space="preserve">
	<!-- enable-background="new 0 0 204.891 151.016" -->

	<defs>
		<pattern id="clc-pattern-unit" x="0" y="0" width="204.891" height="151.016" patternUnits="userSpaceOnUse">
			<path class="clc-pattern-unit-letter" d="M3.081,21.337c0-1.359,0.214-2.641,0.641-3.84c0.426-1.2,1.08-2.246,1.96-3.141c0.88-0.893,1.993-1.6,3.341-2.119
				c1.346-0.521,2.953-0.78,4.819-0.78c1.12,0,2.134,0.079,3.04,0.239s1.826,0.428,2.76,0.801l-1.08,3.96
				c-0.533-0.187-1.127-0.354-1.779-0.5c-0.654-0.146-1.487-0.22-2.5-0.22c-1.174,0-2.154,0.14-2.94,0.42
				c-0.787,0.279-1.42,0.666-1.899,1.16c-0.48,0.493-0.828,1.086-1.041,1.779s-0.32,1.439-0.32,2.24
				c0,1.734,0.486,3.094,1.461,4.08c0.973,0.986,2.633,1.48,4.98,1.48c0.772,0,1.579-0.054,2.42-0.16
				c0.84-0.107,1.605-0.28,2.299-0.521l0.721,4.04c-0.693,0.268-1.533,0.486-2.52,0.66c-0.987,0.173-2.147,0.26-3.48,0.26
				c-1.92,0-3.566-0.254-4.939-0.76c-1.375-0.506-2.5-1.2-3.381-2.08c-0.88-0.88-1.527-1.92-1.939-3.12
				C3.288,24.017,3.081,22.724,3.081,21.337z"></path>
			<path class="clc-pattern-unit-letter" d="M40.002,29.937c-0.08,0.055-0.261,0.146-0.54,0.28c-0.28,0.134-0.646,0.274-1.1,0.42
				c-0.454,0.146-1.008,0.273-1.66,0.38c-0.654,0.106-1.395,0.16-2.221,0.16c-2.267,0-3.92-0.673-4.959-2.02
				c-1.041-1.347-1.561-3.313-1.561-5.9V7.297h-5.2v-4.08h10.12v20.399c0,1.281,0.253,2.147,0.76,2.601
				c0.506,0.454,1.146,0.681,1.92,0.681c0.986,0,1.813-0.134,2.48-0.4c0.666-0.267,1.105-0.44,1.32-0.52L40.002,29.937z"></path>
			<path class="clc-pattern-unit-letter" d="M43.081,21.337c0-1.359,0.214-2.641,0.641-3.84c0.426-1.2,1.08-2.246,1.96-3.141c0.88-0.893,1.993-1.6,3.341-2.119
				c1.346-0.521,2.953-0.78,4.819-0.78c1.12,0,2.134,0.079,3.04,0.239s1.826,0.428,2.76,0.801l-1.08,3.96
				c-0.533-0.187-1.127-0.354-1.779-0.5c-0.654-0.146-1.487-0.22-2.5-0.22c-1.174,0-2.154,0.14-2.94,0.42
				c-0.787,0.279-1.42,0.666-1.899,1.16c-0.48,0.493-0.828,1.086-1.041,1.779s-0.32,1.439-0.32,2.24
				c0,1.734,0.486,3.094,1.461,4.08c0.973,0.986,2.633,1.48,4.98,1.48c0.772,0,1.579-0.054,2.42-0.16
				c0.84-0.107,1.605-0.28,2.299-0.521l0.721,4.04c-0.693,0.268-1.533,0.486-2.52,0.66c-0.987,0.173-2.147,0.26-3.48,0.26
				c-1.92,0-3.566-0.254-4.939-0.76c-1.375-0.506-2.5-1.2-3.381-2.08c-0.88-0.88-1.527-1.92-1.939-3.12
				C43.288,24.017,43.081,22.724,43.081,21.337z"></path>
			<path class="clc-pattern-unit-letter" d="M80.002,29.937c-0.08,0.055-0.261,0.146-0.54,0.28c-0.28,0.134-0.646,0.274-1.1,0.42
				c-0.454,0.146-1.008,0.273-1.66,0.38c-0.654,0.106-1.395,0.16-2.221,0.16c-2.267,0-3.92-0.673-4.959-2.02
				c-1.041-1.347-1.561-3.313-1.561-5.9V7.297h-5.2v-4.08h10.12v20.399c0,1.281,0.253,2.147,0.76,2.601
				c0.506,0.454,1.146,0.681,1.92,0.681c0.986,0,1.813-0.134,2.48-0.4c0.666-0.267,1.105-0.44,1.32-0.52L80.002,29.937z"></path>
			<path class="clc-pattern-unit-letter" d="M83.081,21.337c0-1.359,0.214-2.641,0.641-3.84c0.426-1.2,1.08-2.246,1.96-3.141c0.88-0.893,1.993-1.6,3.341-2.119
				c1.346-0.521,2.953-0.78,4.82-0.78c1.119,0,2.133,0.079,3.039,0.239s1.826,0.428,2.76,0.801l-1.08,3.96
				c-0.533-0.187-1.127-0.354-1.779-0.5c-0.654-0.146-1.486-0.22-2.5-0.22c-1.174,0-2.154,0.14-2.939,0.42
				c-0.787,0.279-1.42,0.666-1.9,1.16c-0.48,0.493-0.828,1.086-1.041,1.779s-0.32,1.439-0.32,2.24c0,1.734,0.486,3.094,1.461,4.08
				c0.973,0.986,2.633,1.48,4.98,1.48c0.773,0,1.58-0.054,2.42-0.16c0.84-0.107,1.605-0.28,2.299-0.521l0.721,4.04
				c-0.693,0.268-1.533,0.486-2.52,0.66c-0.986,0.173-2.146,0.26-3.48,0.26c-1.92,0-3.566-0.254-4.939-0.76
				c-1.375-0.506-2.5-1.2-3.381-2.08c-0.88-0.88-1.527-1.92-1.939-3.12C83.288,24.017,83.081,22.724,83.081,21.337z"></path>
			<path class="clc-pattern-unit-letter" d="M201.235,25.217c-0.412,1.2-1.061,2.24-1.939,3.12c-0.881,0.88-2.006,1.574-3.381,2.08
				c-1.373,0.506-3.02,0.76-4.939,0.76c-1.334,0-2.494-0.087-3.48-0.26c-0.986-0.174-1.826-0.393-2.52-0.66l0.721-4.04
				c0.693,0.24,1.459,0.413,2.299,0.521c0.84,0.106,1.646,0.16,2.42,0.16c2.348,0,4.008-0.494,4.98-1.48
				c0.975-0.986,1.461-2.346,1.461-4.08c0-0.801-0.107-1.547-0.32-2.24s-0.561-1.286-1.041-1.779c-0.48-0.494-1.113-0.881-1.9-1.16
				c-0.785-0.28-1.766-0.42-2.939-0.42c-1.014,0-1.846,0.073-2.5,0.22c-0.652,0.146-1.246,0.313-1.779,0.5l-1.08-3.96
				c0.934-0.373,1.854-0.641,2.76-0.801s1.92-0.239,3.039-0.239c1.867,0,3.475,0.26,4.82,0.78c1.348,0.52,2.461,1.227,3.34,2.119
				c0.881,0.895,1.535,1.94,1.961,3.141c0.426,1.199,0.641,2.48,0.641,3.84C201.856,22.724,201.649,24.017,201.235,25.217z"></path>
			<path class="clc-pattern-unit-letter" d="M165.575,25.978c0.215,0.079,0.654,0.253,1.32,0.52s1.494,0.4,2.48,0.4c0.773,0,1.414-0.227,1.92-0.681
				c0.506-0.453,0.76-1.319,0.76-2.601V3.217h10.119v4.08h-5.199v15.96c0,2.587-0.52,4.554-1.561,5.9
				c-1.039,1.347-2.693,2.02-4.959,2.02c-0.826,0-1.566-0.054-2.221-0.16c-0.652-0.106-1.207-0.233-1.66-0.38
				c-0.453-0.146-0.82-0.286-1.1-0.42s-0.461-0.226-0.541-0.28L165.575,25.978z"></path>
			<path class="clc-pattern-unit-letter" d="M161.235,25.217c-0.412,1.2-1.061,2.24-1.939,3.12c-0.881,0.88-2.006,1.574-3.381,2.08
				c-1.373,0.506-3.02,0.76-4.939,0.76c-1.334,0-2.494-0.087-3.48-0.26c-0.986-0.174-1.826-0.393-2.52-0.66l0.721-4.04
				c0.693,0.24,1.459,0.413,2.299,0.521c0.84,0.106,1.646,0.16,2.42,0.16c2.348,0,4.008-0.494,4.98-1.48
				c0.975-0.986,1.461-2.346,1.461-4.08c0-0.801-0.107-1.547-0.32-2.24s-0.561-1.286-1.041-1.779c-0.48-0.494-1.113-0.881-1.9-1.16
				c-0.785-0.28-1.766-0.42-2.939-0.42c-1.014,0-1.846,0.073-2.5,0.22c-0.652,0.146-1.246,0.313-1.779,0.5l-1.08-3.96
				c0.934-0.373,1.854-0.641,2.76-0.801s1.92-0.239,3.039-0.239c1.867,0,3.475,0.26,4.82,0.78c1.348,0.52,2.461,1.227,3.34,2.119
				c0.881,0.895,1.535,1.94,1.961,3.141c0.426,1.199,0.641,2.48,0.641,3.84C161.856,22.724,161.649,24.017,161.235,25.217z"></path>
			<path class="clc-pattern-unit-letter" d="M125.575,25.978c0.215,0.079,0.654,0.253,1.32,0.52s1.494,0.4,2.48,0.4c0.773,0,1.414-0.227,1.92-0.681
				c0.506-0.453,0.76-1.319,0.76-2.601V3.217h10.119v4.08h-5.199v15.96c0,2.587-0.52,4.554-1.561,5.9
				c-1.039,1.347-2.693,2.02-4.959,2.02c-0.826,0-1.566-0.054-2.221-0.16c-0.652-0.106-1.207-0.233-1.66-0.38
				c-0.453-0.146-0.82-0.286-1.1-0.42s-0.461-0.226-0.541-0.28L125.575,25.978z"></path>
			<path class="clc-pattern-unit-letter" d="M121.235,25.217c-0.412,1.2-1.061,2.24-1.939,3.12c-0.881,0.88-2.006,1.574-3.381,2.08
				c-1.373,0.506-3.02,0.76-4.939,0.76c-1.334,0-2.494-0.087-3.48-0.26c-0.986-0.174-1.826-0.393-2.52-0.66l0.721-4.04
				c0.693,0.24,1.459,0.413,2.299,0.521c0.84,0.106,1.646,0.16,2.42,0.16c2.348,0,4.008-0.494,4.98-1.48
				c0.975-0.986,1.461-2.346,1.461-4.08c0-0.801-0.107-1.547-0.32-2.24s-0.561-1.286-1.041-1.779c-0.48-0.494-1.113-0.881-1.9-1.16
				c-0.785-0.28-1.766-0.42-2.939-0.42c-1.014,0-1.846,0.073-2.5,0.22c-0.652,0.146-1.246,0.313-1.779,0.5l-1.08-3.96
				c0.934-0.373,1.854-0.641,2.76-0.801s1.92-0.239,3.039-0.239c1.867,0,3.475,0.26,4.82,0.78c1.348,0.52,2.461,1.227,3.34,2.119
				c0.881,0.895,1.535,1.94,1.961,3.141c0.426,1.199,0.641,2.48,0.641,3.84C121.856,22.724,121.649,24.017,121.235,25.217z"></path>
			<path class="clc-pattern-unit-letter" d="M3.081,21.337c0-1.359,0.214-2.641,0.641-3.84c0.426-1.2,1.08-2.246,1.96-3.141c0.88-0.893,1.993-1.6,3.341-2.119
				c1.346-0.521,2.953-0.78,4.819-0.78c1.12,0,2.134,0.079,3.04,0.239s1.826,0.428,2.76,0.801l-1.08,3.96
				c-0.533-0.187-1.127-0.354-1.779-0.5c-0.654-0.146-1.487-0.22-2.5-0.22c-1.174,0-2.154,0.14-2.94,0.42
				c-0.787,0.279-1.42,0.666-1.899,1.16c-0.48,0.493-0.828,1.086-1.041,1.779s-0.32,1.439-0.32,2.24
				c0,1.734,0.486,3.094,1.461,4.08c0.973,0.986,2.633,1.48,4.98,1.48c0.772,0,1.579-0.054,2.42-0.16
				c0.84-0.107,1.605-0.28,2.299-0.521l0.721,4.04c-0.693,0.268-1.533,0.486-2.52,0.66c-0.987,0.173-2.147,0.26-3.48,0.26
				c-1.92,0-3.566-0.254-4.939-0.76c-1.375-0.506-2.5-1.2-3.381-2.08c-0.88-0.88-1.527-1.92-1.939-3.12
				C3.288,24.017,3.081,22.724,3.081,21.337z"></path>
			<path class="clc-pattern-unit-letter" d="M40.002,29.937c-0.08,0.055-0.261,0.146-0.54,0.28c-0.28,0.134-0.646,0.274-1.1,0.42
				c-0.454,0.146-1.008,0.273-1.66,0.38c-0.654,0.106-1.395,0.16-2.221,0.16c-2.267,0-3.92-0.673-4.959-2.02
				c-1.041-1.347-1.561-3.313-1.561-5.9V7.297h-5.2v-4.08h10.12v20.399c0,1.281,0.253,2.147,0.76,2.601
				c0.506,0.454,1.146,0.681,1.92,0.681c0.986,0,1.813-0.134,2.48-0.4c0.666-0.267,1.105-0.44,1.32-0.52L40.002,29.937z"></path>
			<path class="clc-pattern-unit-letter" d="M43.081,21.337c0-1.359,0.214-2.641,0.641-3.84c0.426-1.2,1.08-2.246,1.96-3.141c0.88-0.893,1.993-1.6,3.341-2.119
				c1.346-0.521,2.953-0.78,4.819-0.78c1.12,0,2.134,0.079,3.04,0.239s1.826,0.428,2.76,0.801l-1.08,3.96
				c-0.533-0.187-1.127-0.354-1.779-0.5c-0.654-0.146-1.487-0.22-2.5-0.22c-1.174,0-2.154,0.14-2.94,0.42
				c-0.787,0.279-1.42,0.666-1.899,1.16c-0.48,0.493-0.828,1.086-1.041,1.779s-0.32,1.439-0.32,2.24
				c0,1.734,0.486,3.094,1.461,4.08c0.973,0.986,2.633,1.48,4.98,1.48c0.772,0,1.579-0.054,2.42-0.16
				c0.84-0.107,1.605-0.28,2.299-0.521l0.721,4.04c-0.693,0.268-1.533,0.486-2.52,0.66c-0.987,0.173-2.147,0.26-3.48,0.26
				c-1.92,0-3.566-0.254-4.939-0.76c-1.375-0.506-2.5-1.2-3.381-2.08c-0.88-0.88-1.527-1.92-1.939-3.12
				C43.288,24.017,43.081,22.724,43.081,21.337z"></path>
			<path class="clc-pattern-unit-letter" d="M80.002,29.937c-0.08,0.055-0.261,0.146-0.54,0.28c-0.28,0.134-0.646,0.274-1.1,0.42
				c-0.454,0.146-1.008,0.273-1.66,0.38c-0.654,0.106-1.395,0.16-2.221,0.16c-2.267,0-3.92-0.673-4.959-2.02
				c-1.041-1.347-1.561-3.313-1.561-5.9V7.297h-5.2v-4.08h10.12v20.399c0,1.281,0.253,2.147,0.76,2.601
				c0.506,0.454,1.146,0.681,1.92,0.681c0.986,0,1.813-0.134,2.48-0.4c0.666-0.267,1.105-0.44,1.32-0.52L80.002,29.937z"></path>
			<path class="clc-pattern-unit-letter" d="M83.081,21.337c0-1.359,0.214-2.641,0.641-3.84c0.426-1.2,1.08-2.246,1.96-3.141c0.88-0.893,1.993-1.6,3.341-2.119
				c1.346-0.521,2.953-0.78,4.82-0.78c1.119,0,2.133,0.079,3.039,0.239s1.826,0.428,2.76,0.801l-1.08,3.96
				c-0.533-0.187-1.127-0.354-1.779-0.5c-0.654-0.146-1.486-0.22-2.5-0.22c-1.174,0-2.154,0.14-2.939,0.42
				c-0.787,0.279-1.42,0.666-1.9,1.16c-0.48,0.493-0.828,1.086-1.041,1.779s-0.32,1.439-0.32,2.24c0,1.734,0.486,3.094,1.461,4.08
				c0.973,0.986,2.633,1.48,4.98,1.48c0.773,0,1.58-0.054,2.42-0.16c0.84-0.107,1.605-0.28,2.299-0.521l0.721,4.04
				c-0.693,0.268-1.533,0.486-2.52,0.66c-0.986,0.173-2.146,0.26-3.48,0.26c-1.92,0-3.566-0.254-4.939-0.76
				c-1.375-0.506-2.5-1.2-3.381-2.08c-0.88-0.88-1.527-1.92-1.939-3.12C83.288,24.017,83.081,22.724,83.081,21.337z"></path>
			<path class="clc-pattern-unit-letter" d="M201.235,25.217c-0.412,1.2-1.061,2.24-1.939,3.12c-0.881,0.88-2.006,1.574-3.381,2.08
				c-1.373,0.506-3.02,0.76-4.939,0.76c-1.334,0-2.494-0.087-3.48-0.26c-0.986-0.174-1.826-0.393-2.52-0.66l0.721-4.04
				c0.693,0.24,1.459,0.413,2.299,0.521c0.84,0.106,1.646,0.16,2.42,0.16c2.348,0,4.008-0.494,4.98-1.48
				c0.975-0.986,1.461-2.346,1.461-4.08c0-0.801-0.107-1.547-0.32-2.24s-0.561-1.286-1.041-1.779c-0.48-0.494-1.113-0.881-1.9-1.16
				c-0.785-0.28-1.766-0.42-2.939-0.42c-1.014,0-1.846,0.073-2.5,0.22c-0.652,0.146-1.246,0.313-1.779,0.5l-1.08-3.96
				c0.934-0.373,1.854-0.641,2.76-0.801s1.92-0.239,3.039-0.239c1.867,0,3.475,0.26,4.82,0.78c1.348,0.52,2.461,1.227,3.34,2.119
				c0.881,0.895,1.535,1.94,1.961,3.141c0.426,1.199,0.641,2.48,0.641,3.84C201.856,22.724,201.649,24.017,201.235,25.217z"></path>
			<path class="clc-pattern-unit-letter" d="M165.575,25.978c0.215,0.079,0.654,0.253,1.32,0.52s1.494,0.4,2.48,0.4c0.773,0,1.414-0.227,1.92-0.681
				c0.506-0.453,0.76-1.319,0.76-2.601V3.217h10.119v4.08h-5.199v15.96c0,2.587-0.52,4.554-1.561,5.9
				c-1.039,1.347-2.693,2.02-4.959,2.02c-0.826,0-1.566-0.054-2.221-0.16c-0.652-0.106-1.207-0.233-1.66-0.38
				c-0.453-0.146-0.82-0.286-1.1-0.42s-0.461-0.226-0.541-0.28L165.575,25.978z"></path>
			<path class="clc-pattern-unit-letter" d="M161.235,25.217c-0.412,1.2-1.061,2.24-1.939,3.12c-0.881,0.88-2.006,1.574-3.381,2.08
				c-1.373,0.506-3.02,0.76-4.939,0.76c-1.334,0-2.494-0.087-3.48-0.26c-0.986-0.174-1.826-0.393-2.52-0.66l0.721-4.04
				c0.693,0.24,1.459,0.413,2.299,0.521c0.84,0.106,1.646,0.16,2.42,0.16c2.348,0,4.008-0.494,4.98-1.48
				c0.975-0.986,1.461-2.346,1.461-4.08c0-0.801-0.107-1.547-0.32-2.24s-0.561-1.286-1.041-1.779c-0.48-0.494-1.113-0.881-1.9-1.16
				c-0.785-0.28-1.766-0.42-2.939-0.42c-1.014,0-1.846,0.073-2.5,0.22c-0.652,0.146-1.246,0.313-1.779,0.5l-1.08-3.96
				c0.934-0.373,1.854-0.641,2.76-0.801s1.92-0.239,3.039-0.239c1.867,0,3.475,0.26,4.82,0.78c1.348,0.52,2.461,1.227,3.34,2.119
				c0.881,0.895,1.535,1.94,1.961,3.141c0.426,1.199,0.641,2.48,0.641,3.84C161.856,22.724,161.649,24.017,161.235,25.217z"></path>
			<path class="clc-pattern-unit-letter" d="M125.575,25.978c0.215,0.079,0.654,0.253,1.32,0.52s1.494,0.4,2.48,0.4c0.773,0,1.414-0.227,1.92-0.681
				c0.506-0.453,0.76-1.319,0.76-2.601V3.217h10.119v4.08h-5.199v15.96c0,2.587-0.52,4.554-1.561,5.9
				c-1.039,1.347-2.693,2.02-4.959,2.02c-0.826,0-1.566-0.054-2.221-0.16c-0.652-0.106-1.207-0.233-1.66-0.38
				c-0.453-0.146-0.82-0.286-1.1-0.42s-0.461-0.226-0.541-0.28L125.575,25.978z"></path>
			<path class="clc-pattern-unit-letter" d="M121.235,25.217c-0.412,1.2-1.061,2.24-1.939,3.12c-0.881,0.88-2.006,1.574-3.381,2.08
				c-1.373,0.506-3.02,0.76-4.939,0.76c-1.334,0-2.494-0.087-3.48-0.26c-0.986-0.174-1.826-0.393-2.52-0.66l0.721-4.04
				c0.693,0.24,1.459,0.413,2.299,0.521c0.84,0.106,1.646,0.16,2.42,0.16c2.348,0,4.008-0.494,4.98-1.48
				c0.975-0.986,1.461-2.346,1.461-4.08c0-0.801-0.107-1.547-0.32-2.24s-0.561-1.286-1.041-1.779c-0.48-0.494-1.113-0.881-1.9-1.16
				c-0.785-0.28-1.766-0.42-2.939-0.42c-1.014,0-1.846,0.073-2.5,0.22c-0.652,0.146-1.246,0.313-1.779,0.5l-1.08-3.96
				c0.934-0.373,1.854-0.641,2.76-0.801s1.92-0.239,3.039-0.239c1.867,0,3.475,0.26,4.82,0.78c1.348,0.52,2.461,1.227,3.34,2.119
				c0.881,0.895,1.535,1.94,1.961,3.141c0.426,1.199,0.641,2.48,0.641,3.84C121.856,22.724,121.649,24.017,121.235,25.217z"></path>
			<path class="clc-pattern-unit-letter" d="M20.002,68.807c-0.08,0.055-0.261,0.146-0.54,0.28c-0.28,0.134-0.646,0.274-1.1,0.42
				c-0.454,0.146-1.008,0.273-1.66,0.38c-0.654,0.106-1.395,0.16-2.221,0.16c-2.267,0-3.92-0.673-4.959-2.02
				c-1.041-1.347-1.561-3.313-1.561-5.9v-15.96h-5.2v-4.08h10.12v20.399c0,1.281,0.253,2.147,0.76,2.601
				c0.506,0.454,1.146,0.681,1.92,0.681c0.986,0,1.813-0.134,2.48-0.4c0.666-0.267,1.105-0.44,1.32-0.52L20.002,68.807z"></path>
			<path class="clc-pattern-unit-letter" d="M23.081,60.208c0-1.359,0.214-2.641,0.641-3.84c0.426-1.2,1.08-2.246,1.96-3.141c0.88-0.893,1.993-1.6,3.341-2.119
				c1.346-0.521,2.953-0.78,4.819-0.78c1.12,0,2.134,0.079,3.04,0.239s1.826,0.428,2.76,0.801l-1.08,3.96
				c-0.533-0.187-1.127-0.354-1.779-0.5c-0.654-0.146-1.487-0.22-2.5-0.22c-1.174,0-2.154,0.14-2.94,0.42
				c-0.787,0.279-1.42,0.666-1.899,1.16c-0.48,0.493-0.828,1.086-1.041,1.779s-0.32,1.439-0.32,2.24
				c0,1.734,0.486,3.094,1.461,4.08c0.973,0.986,2.633,1.48,4.98,1.48c0.772,0,1.579-0.054,2.42-0.16
				c0.84-0.107,1.605-0.28,2.299-0.521l0.721,4.04c-0.693,0.268-1.533,0.486-2.52,0.66c-0.987,0.173-2.147,0.26-3.48,0.26
				c-1.92,0-3.566-0.254-4.939-0.76c-1.375-0.506-2.5-1.2-3.381-2.08c-0.88-0.88-1.527-1.92-1.939-3.12
				C23.288,62.887,23.081,61.594,23.081,60.208z"></path>
			<path class="clc-pattern-unit-letter" d="M60.002,68.807c-0.08,0.055-0.261,0.146-0.54,0.28c-0.28,0.134-0.646,0.274-1.1,0.42
				c-0.454,0.146-1.008,0.273-1.66,0.38c-0.654,0.106-1.395,0.16-2.221,0.16c-2.267,0-3.92-0.673-4.959-2.02
				c-1.041-1.347-1.561-3.313-1.561-5.9v-15.96h-5.2v-4.08h10.12v20.399c0,1.281,0.253,2.147,0.76,2.601
				c0.506,0.454,1.146,0.681,1.92,0.681c0.986,0,1.813-0.134,2.48-0.4c0.666-0.267,1.105-0.44,1.32-0.52L60.002,68.807z"></path>
			<path class="clc-pattern-unit-letter" d="M63.081,60.208c0-1.359,0.214-2.641,0.641-3.84c0.426-1.2,1.08-2.246,1.96-3.141c0.88-0.893,1.993-1.6,3.341-2.119
				c1.346-0.521,2.953-0.78,4.819-0.78c1.12,0,2.134,0.079,3.04,0.239s1.826,0.428,2.76,0.801l-1.08,3.96
				c-0.533-0.187-1.127-0.354-1.779-0.5c-0.654-0.146-1.487-0.22-2.5-0.22c-1.174,0-2.154,0.14-2.94,0.42
				c-0.787,0.279-1.42,0.666-1.899,1.16c-0.48,0.493-0.828,1.086-1.041,1.779s-0.32,1.439-0.32,2.24
				c0,1.734,0.486,3.094,1.461,4.08c0.973,0.986,2.633,1.48,4.98,1.48c0.772,0,1.579-0.054,2.42-0.16
				c0.84-0.107,1.605-0.28,2.299-0.521l0.721,4.04c-0.693,0.268-1.533,0.486-2.52,0.66c-0.987,0.173-2.147,0.26-3.48,0.26
				c-1.92,0-3.566-0.254-4.939-0.76c-1.375-0.506-2.5-1.2-3.381-2.08c-0.88-0.88-1.527-1.92-1.939-3.12
				C63.288,62.887,63.081,61.594,63.081,60.208z"></path>
			<path class="clc-pattern-unit-letter" d="M100.003,68.807c-0.08,0.055-0.262,0.146-0.541,0.28s-0.646,0.274-1.1,0.42c-0.453,0.146-1.008,0.273-1.66,0.38
				c-0.654,0.106-1.395,0.16-2.221,0.16c-2.266,0-3.92-0.673-4.959-2.02c-1.041-1.347-1.561-3.313-1.561-5.9v-15.96h-5.2v-4.08
				h10.12v20.399c0,1.281,0.254,2.147,0.76,2.601c0.506,0.454,1.146,0.681,1.92,0.681c0.986,0,1.814-0.134,2.48-0.4
				s1.105-0.44,1.32-0.52L100.003,68.807z"></path>
			<path class="clc-pattern-unit-letter" d="M185.575,64.848c0.215,0.079,0.654,0.253,1.32,0.52s1.494,0.4,2.48,0.4c0.773,0,1.414-0.227,1.92-0.681
				c0.506-0.453,0.76-1.319,0.76-2.601V42.087h10.119v4.08h-5.199v15.96c0,2.587-0.52,4.554-1.561,5.9
				c-1.039,1.347-2.693,2.02-4.959,2.02c-0.826,0-1.566-0.054-2.221-0.16c-0.652-0.106-1.207-0.233-1.66-0.38
				c-0.453-0.146-0.82-0.286-1.1-0.42s-0.461-0.226-0.541-0.28L185.575,64.848z"></path>
			<path class="clc-pattern-unit-letter" d="M181.235,64.087c-0.412,1.2-1.061,2.24-1.939,3.12c-0.881,0.88-2.006,1.574-3.381,2.08
				c-1.373,0.506-3.02,0.76-4.939,0.76c-1.334,0-2.494-0.087-3.48-0.26c-0.986-0.174-1.826-0.393-2.52-0.66l0.721-4.04
				c0.693,0.24,1.459,0.413,2.299,0.521c0.84,0.106,1.646,0.16,2.42,0.16c2.348,0,4.008-0.494,4.98-1.48
				c0.975-0.986,1.461-2.346,1.461-4.08c0-0.801-0.107-1.547-0.32-2.24s-0.561-1.286-1.041-1.779c-0.48-0.494-1.113-0.881-1.9-1.16
				c-0.785-0.28-1.766-0.42-2.939-0.42c-1.014,0-1.846,0.073-2.5,0.22c-0.652,0.146-1.246,0.313-1.779,0.5l-1.08-3.96
				c0.934-0.373,1.854-0.641,2.76-0.801s1.92-0.239,3.039-0.239c1.867,0,3.475,0.26,4.82,0.78c1.348,0.52,2.461,1.227,3.34,2.119
				c0.881,0.895,1.535,1.94,1.961,3.141c0.426,1.199,0.641,2.48,0.641,3.84C181.856,61.594,181.649,62.887,181.235,64.087z"></path>
			<path class="clc-pattern-unit-letter" d="M145.575,64.848c0.215,0.079,0.654,0.253,1.32,0.52s1.494,0.4,2.48,0.4c0.773,0,1.414-0.227,1.92-0.681
				c0.506-0.453,0.76-1.319,0.76-2.601V42.087h10.119v4.08h-5.199v15.96c0,2.587-0.52,4.554-1.561,5.9
				c-1.039,1.347-2.693,2.02-4.959,2.02c-0.826,0-1.566-0.054-2.221-0.16c-0.652-0.106-1.207-0.233-1.66-0.38
				c-0.453-0.146-0.82-0.286-1.1-0.42s-0.461-0.226-0.541-0.28L145.575,64.848z"></path>
			<path class="clc-pattern-unit-letter" d="M141.235,64.087c-0.412,1.2-1.061,2.24-1.939,3.12c-0.881,0.88-2.006,1.574-3.381,2.08
				c-1.373,0.506-3.02,0.76-4.939,0.76c-1.334,0-2.494-0.087-3.48-0.26c-0.986-0.174-1.826-0.393-2.52-0.66l0.721-4.04
				c0.693,0.24,1.459,0.413,2.299,0.521c0.84,0.106,1.646,0.16,2.42,0.16c2.348,0,4.008-0.494,4.98-1.48
				c0.975-0.986,1.461-2.346,1.461-4.08c0-0.801-0.107-1.547-0.32-2.24s-0.561-1.286-1.041-1.779c-0.48-0.494-1.113-0.881-1.9-1.16
				c-0.785-0.28-1.766-0.42-2.939-0.42c-1.014,0-1.846,0.073-2.5,0.22c-0.652,0.146-1.246,0.313-1.779,0.5l-1.08-3.96
				c0.934-0.373,1.854-0.641,2.76-0.801s1.92-0.239,3.039-0.239c1.867,0,3.475,0.26,4.82,0.78c1.348,0.52,2.461,1.227,3.34,2.119
				c0.881,0.895,1.535,1.94,1.961,3.141c0.426,1.199,0.641,2.48,0.641,3.84C141.856,61.594,141.649,62.887,141.235,64.087z"></path>
			<path class="clc-pattern-unit-letter" d="M105.575,64.848c0.215,0.079,0.654,0.253,1.32,0.52s1.494,0.4,2.48,0.4c0.773,0,1.414-0.227,1.92-0.681
				c0.506-0.453,0.76-1.319,0.76-2.601V42.087h10.119v4.08h-5.199v15.96c0,2.587-0.52,4.554-1.561,5.9
				c-1.039,1.347-2.693,2.02-4.959,2.02c-0.826,0-1.566-0.054-2.221-0.16c-0.652-0.106-1.207-0.233-1.66-0.38
				c-0.453-0.146-0.82-0.286-1.1-0.42s-0.461-0.226-0.541-0.28L105.575,64.848z"></path>
			<path class="clc-pattern-unit-letter" d="M20.002,68.807c-0.08,0.055-0.261,0.146-0.54,0.28c-0.28,0.134-0.646,0.274-1.1,0.42
				c-0.454,0.146-1.008,0.273-1.66,0.38c-0.654,0.106-1.395,0.16-2.221,0.16c-2.267,0-3.92-0.673-4.959-2.02
				c-1.041-1.347-1.561-3.313-1.561-5.9v-15.96h-5.2v-4.08h10.12v20.399c0,1.281,0.253,2.147,0.76,2.601
				c0.506,0.454,1.146,0.681,1.92,0.681c0.986,0,1.813-0.134,2.48-0.4c0.666-0.267,1.105-0.44,1.32-0.52L20.002,68.807z"></path>
			<path class="clc-pattern-unit-letter" d="M23.081,60.208c0-1.359,0.214-2.641,0.641-3.84c0.426-1.2,1.08-2.246,1.96-3.141c0.88-0.893,1.993-1.6,3.341-2.119
				c1.346-0.521,2.953-0.78,4.819-0.78c1.12,0,2.134,0.079,3.04,0.239s1.826,0.428,2.76,0.801l-1.08,3.96
				c-0.533-0.187-1.127-0.354-1.779-0.5c-0.654-0.146-1.487-0.22-2.5-0.22c-1.174,0-2.154,0.14-2.94,0.42
				c-0.787,0.279-1.42,0.666-1.899,1.16c-0.48,0.493-0.828,1.086-1.041,1.779s-0.32,1.439-0.32,2.24
				c0,1.734,0.486,3.094,1.461,4.08c0.973,0.986,2.633,1.48,4.98,1.48c0.772,0,1.579-0.054,2.42-0.16
				c0.84-0.107,1.605-0.28,2.299-0.521l0.721,4.04c-0.693,0.268-1.533,0.486-2.52,0.66c-0.987,0.173-2.147,0.26-3.48,0.26
				c-1.92,0-3.566-0.254-4.939-0.76c-1.375-0.506-2.5-1.2-3.381-2.08c-0.88-0.88-1.527-1.92-1.939-3.12
				C23.288,62.887,23.081,61.594,23.081,60.208z"></path>
			<path class="clc-pattern-unit-letter" d="M60.002,68.807c-0.08,0.055-0.261,0.146-0.54,0.28c-0.28,0.134-0.646,0.274-1.1,0.42
				c-0.454,0.146-1.008,0.273-1.66,0.38c-0.654,0.106-1.395,0.16-2.221,0.16c-2.267,0-3.92-0.673-4.959-2.02
				c-1.041-1.347-1.561-3.313-1.561-5.9v-15.96h-5.2v-4.08h10.12v20.399c0,1.281,0.253,2.147,0.76,2.601
				c0.506,0.454,1.146,0.681,1.92,0.681c0.986,0,1.813-0.134,2.48-0.4c0.666-0.267,1.105-0.44,1.32-0.52L60.002,68.807z"></path>
			<path class="clc-pattern-unit-letter" d="M63.081,60.208c0-1.359,0.214-2.641,0.641-3.84c0.426-1.2,1.08-2.246,1.96-3.141c0.88-0.893,1.993-1.6,3.341-2.119
				c1.346-0.521,2.953-0.78,4.819-0.78c1.12,0,2.134,0.079,3.04,0.239s1.826,0.428,2.76,0.801l-1.08,3.96
				c-0.533-0.187-1.127-0.354-1.779-0.5c-0.654-0.146-1.487-0.22-2.5-0.22c-1.174,0-2.154,0.14-2.94,0.42
				c-0.787,0.279-1.42,0.666-1.899,1.16c-0.48,0.493-0.828,1.086-1.041,1.779s-0.32,1.439-0.32,2.24
				c0,1.734,0.486,3.094,1.461,4.08c0.973,0.986,2.633,1.48,4.98,1.48c0.772,0,1.579-0.054,2.42-0.16
				c0.84-0.107,1.605-0.28,2.299-0.521l0.721,4.04c-0.693,0.268-1.533,0.486-2.52,0.66c-0.987,0.173-2.147,0.26-3.48,0.26
				c-1.92,0-3.566-0.254-4.939-0.76c-1.375-0.506-2.5-1.2-3.381-2.08c-0.88-0.88-1.527-1.92-1.939-3.12
				C63.288,62.887,63.081,61.594,63.081,60.208z"></path>
			<path class="clc-pattern-unit-letter" d="M100.003,68.807c-0.08,0.055-0.262,0.146-0.541,0.28s-0.646,0.274-1.1,0.42c-0.453,0.146-1.008,0.273-1.66,0.38
				c-0.654,0.106-1.395,0.16-2.221,0.16c-2.266,0-3.92-0.673-4.959-2.02c-1.041-1.347-1.561-3.313-1.561-5.9v-15.96h-5.2v-4.08
				h10.12v20.399c0,1.281,0.254,2.147,0.76,2.601c0.506,0.454,1.146,0.681,1.92,0.681c0.986,0,1.814-0.134,2.48-0.4
				s1.105-0.44,1.32-0.52L100.003,68.807z"></path>
			<path class="clc-pattern-unit-letter" d="M185.575,64.848c0.215,0.079,0.654,0.253,1.32,0.52s1.494,0.4,2.48,0.4c0.773,0,1.414-0.227,1.92-0.681
				c0.506-0.453,0.76-1.319,0.76-2.601V42.087h10.119v4.08h-5.199v15.96c0,2.587-0.52,4.554-1.561,5.9
				c-1.039,1.347-2.693,2.02-4.959,2.02c-0.826,0-1.566-0.054-2.221-0.16c-0.652-0.106-1.207-0.233-1.66-0.38
				c-0.453-0.146-0.82-0.286-1.1-0.42s-0.461-0.226-0.541-0.28L185.575,64.848z"></path>
			<path class="clc-pattern-unit-letter" d="M181.235,64.087c-0.412,1.2-1.061,2.24-1.939,3.12c-0.881,0.88-2.006,1.574-3.381,2.08
				c-1.373,0.506-3.02,0.76-4.939,0.76c-1.334,0-2.494-0.087-3.48-0.26c-0.986-0.174-1.826-0.393-2.52-0.66l0.721-4.04
				c0.693,0.24,1.459,0.413,2.299,0.521c0.84,0.106,1.646,0.16,2.42,0.16c2.348,0,4.008-0.494,4.98-1.48
				c0.975-0.986,1.461-2.346,1.461-4.08c0-0.801-0.107-1.547-0.32-2.24s-0.561-1.286-1.041-1.779c-0.48-0.494-1.113-0.881-1.9-1.16
				c-0.785-0.28-1.766-0.42-2.939-0.42c-1.014,0-1.846,0.073-2.5,0.22c-0.652,0.146-1.246,0.313-1.779,0.5l-1.08-3.96
				c0.934-0.373,1.854-0.641,2.76-0.801s1.92-0.239,3.039-0.239c1.867,0,3.475,0.26,4.82,0.78c1.348,0.52,2.461,1.227,3.34,2.119
				c0.881,0.895,1.535,1.94,1.961,3.141c0.426,1.199,0.641,2.48,0.641,3.84C181.856,61.594,181.649,62.887,181.235,64.087z"></path>
			<path class="clc-pattern-unit-letter" d="M145.575,64.848c0.215,0.079,0.654,0.253,1.32,0.52s1.494,0.4,2.48,0.4c0.773,0,1.414-0.227,1.92-0.681
				c0.506-0.453,0.76-1.319,0.76-2.601V42.087h10.119v4.08h-5.199v15.96c0,2.587-0.52,4.554-1.561,5.9
				c-1.039,1.347-2.693,2.02-4.959,2.02c-0.826,0-1.566-0.054-2.221-0.16c-0.652-0.106-1.207-0.233-1.66-0.38
				c-0.453-0.146-0.82-0.286-1.1-0.42s-0.461-0.226-0.541-0.28L145.575,64.848z"></path>
			<path class="clc-pattern-unit-letter" d="M141.235,64.087c-0.412,1.2-1.061,2.24-1.939,3.12c-0.881,0.88-2.006,1.574-3.381,2.08
				c-1.373,0.506-3.02,0.76-4.939,0.76c-1.334,0-2.494-0.087-3.48-0.26c-0.986-0.174-1.826-0.393-2.52-0.66l0.721-4.04
				c0.693,0.24,1.459,0.413,2.299,0.521c0.84,0.106,1.646,0.16,2.42,0.16c2.348,0,4.008-0.494,4.98-1.48
				c0.975-0.986,1.461-2.346,1.461-4.08c0-0.801-0.107-1.547-0.32-2.24s-0.561-1.286-1.041-1.779c-0.48-0.494-1.113-0.881-1.9-1.16
				c-0.785-0.28-1.766-0.42-2.939-0.42c-1.014,0-1.846,0.073-2.5,0.22c-0.652,0.146-1.246,0.313-1.779,0.5l-1.08-3.96
				c0.934-0.373,1.854-0.641,2.76-0.801s1.92-0.239,3.039-0.239c1.867,0,3.475,0.26,4.82,0.78c1.348,0.52,2.461,1.227,3.34,2.119
				c0.881,0.895,1.535,1.94,1.961,3.141c0.426,1.199,0.641,2.48,0.641,3.84C141.856,61.594,141.649,62.887,141.235,64.087z"></path>
			<path class="clc-pattern-unit-letter" d="M105.575,64.848c0.215,0.079,0.654,0.253,1.32,0.52s1.494,0.4,2.48,0.4c0.773,0,1.414-0.227,1.92-0.681
				c0.506-0.453,0.76-1.319,0.76-2.601V42.087h10.119v4.08h-5.199v15.96c0,2.587-0.52,4.554-1.561,5.9
				c-1.039,1.347-2.693,2.02-4.959,2.02c-0.826,0-1.566-0.054-2.221-0.16c-0.652-0.106-1.207-0.233-1.66-0.38
				c-0.453-0.146-0.82-0.286-1.1-0.42s-0.461-0.226-0.541-0.28L105.575,64.848z"></path>
			<path class="clc-pattern-unit-letter" d="M3.702,125.789c0.412-1.2,1.06-2.24,1.939-3.12c0.881-0.88,2.006-1.574,3.381-2.08c1.373-0.506,3.02-0.76,4.939-0.76
				c1.333,0,2.493,0.087,3.48,0.26c0.986,0.174,1.826,0.393,2.52,0.66l-0.721,4.04c-0.693-0.24-1.459-0.413-2.299-0.521
				c-0.841-0.106-1.647-0.16-2.42-0.16c-2.348,0-4.008,0.494-4.98,1.48c-0.975,0.986-1.461,2.346-1.461,4.08
				c0,0.801,0.107,1.547,0.32,2.24s0.561,1.286,1.041,1.779c0.479,0.494,1.112,0.881,1.899,1.16c0.786,0.28,1.767,0.42,2.94,0.42
				c1.013,0,1.846-0.073,2.5-0.22c0.652-0.146,1.246-0.313,1.779-0.5l1.08,3.96c-0.934,0.373-1.854,0.641-2.76,0.801
				s-1.92,0.239-3.04,0.239c-1.866,0-3.474-0.26-4.819-0.78c-1.348-0.52-2.461-1.227-3.341-2.119
				c-0.88-0.895-1.534-1.94-1.96-3.141c-0.427-1.199-0.641-2.48-0.641-3.84C3.081,128.282,3.288,126.989,3.702,125.789z"></path>
			<path class="clc-pattern-unit-letter" d="M39.362,125.028c-0.215-0.079-0.654-0.253-1.32-0.52c-0.667-0.267-1.494-0.4-2.48-0.4c-0.773,0-1.414,0.227-1.92,0.681
				c-0.507,0.453-0.76,1.319-0.76,2.601v20.399h-10.12v-4.08h5.2v-15.96c0-2.587,0.52-4.554,1.561-5.9
				c1.039-1.347,2.692-2.02,4.959-2.02c0.826,0,1.566,0.054,2.221,0.16c0.652,0.106,1.206,0.233,1.66,0.38
				c0.453,0.146,0.819,0.286,1.1,0.42c0.279,0.134,0.46,0.226,0.54,0.28L39.362,125.028z"></path>
			<path class="clc-pattern-unit-letter" d="M43.702,125.789c0.412-1.2,1.06-2.24,1.939-3.12c0.881-0.88,2.006-1.574,3.381-2.08c1.373-0.506,3.02-0.76,4.939-0.76
				c1.333,0,2.493,0.087,3.48,0.26c0.986,0.174,1.826,0.393,2.52,0.66l-0.721,4.04c-0.693-0.24-1.459-0.413-2.299-0.521
				c-0.841-0.106-1.647-0.16-2.42-0.16c-2.348,0-4.008,0.494-4.98,1.48c-0.975,0.986-1.461,2.346-1.461,4.08
				c0,0.801,0.107,1.547,0.32,2.24s0.561,1.286,1.041,1.779c0.479,0.494,1.112,0.881,1.899,1.16c0.786,0.28,1.767,0.42,2.94,0.42
				c1.013,0,1.846-0.073,2.5-0.22c0.652-0.146,1.246-0.313,1.779-0.5l1.08,3.96c-0.934,0.373-1.854,0.641-2.76,0.801
				s-1.92,0.239-3.04,0.239c-1.866,0-3.474-0.26-4.819-0.78c-1.348-0.52-2.461-1.227-3.341-2.119
				c-0.88-0.895-1.534-1.94-1.96-3.141c-0.427-1.199-0.641-2.48-0.641-3.84C43.081,128.282,43.288,126.989,43.702,125.789z"></path>
			<path class="clc-pattern-unit-letter" d="M79.362,125.028c-0.215-0.079-0.654-0.253-1.32-0.52c-0.667-0.267-1.494-0.4-2.48-0.4c-0.773,0-1.414,0.227-1.92,0.681
				c-0.507,0.453-0.76,1.319-0.76,2.601v20.399h-10.12v-4.08h5.2v-15.96c0-2.587,0.52-4.554,1.561-5.9
				c1.039-1.347,2.692-2.02,4.959-2.02c0.826,0,1.566,0.054,2.221,0.16c0.652,0.106,1.206,0.233,1.66,0.38
				c0.453,0.146,0.819,0.286,1.1,0.42c0.279,0.134,0.46,0.226,0.54,0.28L79.362,125.028z"></path>
			<path class="clc-pattern-unit-letter" d="M83.702,125.789c0.412-1.2,1.06-2.24,1.939-3.12c0.881-0.88,2.006-1.574,3.381-2.08c1.373-0.506,3.02-0.76,4.939-0.76
				c1.334,0,2.494,0.087,3.48,0.26c0.986,0.174,1.826,0.393,2.52,0.66l-0.721,4.04c-0.693-0.24-1.459-0.413-2.299-0.521
				c-0.84-0.106-1.646-0.16-2.42-0.16c-2.348,0-4.008,0.494-4.98,1.48c-0.975,0.986-1.461,2.346-1.461,4.08
				c0,0.801,0.107,1.547,0.32,2.24s0.561,1.286,1.041,1.779c0.48,0.494,1.113,0.881,1.9,1.16c0.785,0.28,1.766,0.42,2.939,0.42
				c1.014,0,1.846-0.073,2.5-0.22c0.652-0.146,1.246-0.313,1.779-0.5l1.08,3.96c-0.934,0.373-1.854,0.641-2.76,0.801
				s-1.92,0.239-3.039,0.239c-1.867,0-3.475-0.26-4.82-0.78c-1.348-0.52-2.461-1.227-3.341-2.119
				c-0.88-0.895-1.534-1.94-1.96-3.141c-0.427-1.199-0.641-2.48-0.641-3.84C83.081,128.282,83.288,126.989,83.702,125.789z"></path>
			<path class="clc-pattern-unit-letter" d="M201.856,129.668c0,1.359-0.215,2.641-0.641,3.84c-0.426,1.2-1.08,2.246-1.961,3.141c-0.879,0.893-1.992,1.6-3.34,2.119
				c-1.346,0.521-2.953,0.78-4.82,0.78c-1.119,0-2.133-0.079-3.039-0.239s-1.826-0.428-2.76-0.801l1.08-3.96
				c0.533,0.187,1.127,0.354,1.779,0.5c0.654,0.146,1.486,0.22,2.5,0.22c1.174,0,2.154-0.14,2.939-0.42
				c0.787-0.279,1.42-0.666,1.9-1.16c0.48-0.493,0.828-1.086,1.041-1.779s0.32-1.439,0.32-2.24c0-1.734-0.486-3.094-1.461-4.08
				c-0.973-0.986-2.633-1.48-4.98-1.48c-0.773,0-1.58,0.054-2.42,0.16c-0.84,0.107-1.605,0.28-2.299,0.521l-0.721-4.04
				c0.693-0.268,1.533-0.486,2.52-0.66c0.986-0.173,2.146-0.26,3.48-0.26c1.92,0,3.566,0.254,4.939,0.76
				c1.375,0.506,2.5,1.2,3.381,2.08c0.879,0.88,1.527,1.92,1.939,3.12C201.649,126.989,201.856,128.282,201.856,129.668z"></path>
			<path class="clc-pattern-unit-letter" d="M164.935,121.069c0.08-0.055,0.262-0.146,0.541-0.28s0.646-0.274,1.1-0.42c0.453-0.146,1.008-0.273,1.66-0.38
				c0.654-0.106,1.395-0.16,2.221-0.16c2.266,0,3.92,0.673,4.959,2.02c1.041,1.347,1.561,3.313,1.561,5.9v15.96h5.199v4.08h-10.119
				v-20.399c0-1.281-0.254-2.147-0.76-2.601c-0.506-0.454-1.146-0.681-1.92-0.681c-0.986,0-1.814,0.134-2.48,0.4
				s-1.105,0.44-1.32,0.52L164.935,121.069z"></path>
			<path class="clc-pattern-unit-letter" d="M161.856,129.668c0,1.359-0.215,2.641-0.641,3.84c-0.426,1.2-1.08,2.246-1.961,3.141c-0.879,0.893-1.992,1.6-3.34,2.119
				c-1.346,0.521-2.953,0.78-4.82,0.78c-1.119,0-2.133-0.079-3.039-0.239s-1.826-0.428-2.76-0.801l1.08-3.96
				c0.533,0.187,1.127,0.354,1.779,0.5c0.654,0.146,1.486,0.22,2.5,0.22c1.174,0,2.154-0.14,2.939-0.42
				c0.787-0.279,1.42-0.666,1.9-1.16c0.48-0.493,0.828-1.086,1.041-1.779s0.32-1.439,0.32-2.24c0-1.734-0.486-3.094-1.461-4.08
				c-0.973-0.986-2.633-1.48-4.98-1.48c-0.773,0-1.58,0.054-2.42,0.16c-0.84,0.107-1.605,0.28-2.299,0.521l-0.721-4.04
				c0.693-0.268,1.533-0.486,2.52-0.66c0.986-0.173,2.146-0.26,3.48-0.26c1.92,0,3.566,0.254,4.939,0.76
				c1.375,0.506,2.5,1.2,3.381,2.08c0.879,0.88,1.527,1.92,1.939,3.12C161.649,126.989,161.856,128.282,161.856,129.668z"></path>
			<path class="clc-pattern-unit-letter" d="M124.935,121.069c0.08-0.055,0.262-0.146,0.541-0.28s0.646-0.274,1.1-0.42c0.453-0.146,1.008-0.273,1.66-0.38
				c0.654-0.106,1.395-0.16,2.221-0.16c2.266,0,3.92,0.673,4.959,2.02c1.041,1.347,1.561,3.313,1.561,5.9v15.96h5.199v4.08h-10.119
				v-20.399c0-1.281-0.254-2.147-0.76-2.601c-0.506-0.454-1.146-0.681-1.92-0.681c-0.986,0-1.814,0.134-2.48,0.4
				s-1.105,0.44-1.32,0.52L124.935,121.069z"></path>
			<path class="clc-pattern-unit-letter" d="M121.856,129.668c0,1.359-0.215,2.641-0.641,3.84c-0.426,1.2-1.08,2.246-1.961,3.141c-0.879,0.893-1.992,1.6-3.34,2.119
				c-1.346,0.521-2.953,0.78-4.82,0.78c-1.119,0-2.133-0.079-3.039-0.239s-1.826-0.428-2.76-0.801l1.08-3.96
				c0.533,0.187,1.127,0.354,1.779,0.5c0.654,0.146,1.486,0.22,2.5,0.22c1.174,0,2.154-0.14,2.939-0.42
				c0.787-0.279,1.42-0.666,1.9-1.16c0.48-0.493,0.828-1.086,1.041-1.779s0.32-1.439,0.32-2.24c0-1.734-0.486-3.094-1.461-4.08
				c-0.973-0.986-2.633-1.48-4.98-1.48c-0.773,0-1.58,0.054-2.42,0.16c-0.84,0.107-1.605,0.28-2.299,0.521l-0.721-4.04
				c0.693-0.268,1.533-0.486,2.52-0.66c0.986-0.173,2.146-0.26,3.48-0.26c1.92,0,3.566,0.254,4.939,0.76
				c1.375,0.506,2.5,1.2,3.381,2.08c0.879,0.88,1.527,1.92,1.939,3.12C121.649,126.989,121.856,128.282,121.856,129.668z"></path>
			<path class="clc-pattern-unit-letter" d="M3.702,125.789c0.412-1.2,1.06-2.24,1.939-3.12c0.881-0.88,2.006-1.574,3.381-2.08c1.373-0.506,3.02-0.76,4.939-0.76
				c1.333,0,2.493,0.087,3.48,0.26c0.986,0.174,1.826,0.393,2.52,0.66l-0.721,4.04c-0.693-0.24-1.459-0.413-2.299-0.521
				c-0.841-0.106-1.647-0.16-2.42-0.16c-2.348,0-4.008,0.494-4.98,1.48c-0.975,0.986-1.461,2.346-1.461,4.08
				c0,0.801,0.107,1.547,0.32,2.24s0.561,1.286,1.041,1.779c0.479,0.494,1.112,0.881,1.899,1.16c0.786,0.28,1.767,0.42,2.94,0.42
				c1.013,0,1.846-0.073,2.5-0.22c0.652-0.146,1.246-0.313,1.779-0.5l1.08,3.96c-0.934,0.373-1.854,0.641-2.76,0.801
				s-1.92,0.239-3.04,0.239c-1.866,0-3.474-0.26-4.819-0.78c-1.348-0.52-2.461-1.227-3.341-2.119
				c-0.88-0.895-1.534-1.94-1.96-3.141c-0.427-1.199-0.641-2.48-0.641-3.84C3.081,128.282,3.288,126.989,3.702,125.789z"></path>
			<path class="clc-pattern-unit-letter" d="M39.362,125.028c-0.215-0.079-0.654-0.253-1.32-0.52c-0.667-0.267-1.494-0.4-2.48-0.4c-0.773,0-1.414,0.227-1.92,0.681
				c-0.507,0.453-0.76,1.319-0.76,2.601v20.399h-10.12v-4.08h5.2v-15.96c0-2.587,0.52-4.554,1.561-5.9
				c1.039-1.347,2.692-2.02,4.959-2.02c0.826,0,1.566,0.054,2.221,0.16c0.652,0.106,1.206,0.233,1.66,0.38
				c0.453,0.146,0.819,0.286,1.1,0.42c0.279,0.134,0.46,0.226,0.54,0.28L39.362,125.028z"></path>
			<path class="clc-pattern-unit-letter" d="M43.702,125.789c0.412-1.2,1.06-2.24,1.939-3.12c0.881-0.88,2.006-1.574,3.381-2.08c1.373-0.506,3.02-0.76,4.939-0.76
				c1.333,0,2.493,0.087,3.48,0.26c0.986,0.174,1.826,0.393,2.52,0.66l-0.721,4.04c-0.693-0.24-1.459-0.413-2.299-0.521
				c-0.841-0.106-1.647-0.16-2.42-0.16c-2.348,0-4.008,0.494-4.98,1.48c-0.975,0.986-1.461,2.346-1.461,4.08
				c0,0.801,0.107,1.547,0.32,2.24s0.561,1.286,1.041,1.779c0.479,0.494,1.112,0.881,1.899,1.16c0.786,0.28,1.767,0.42,2.94,0.42
				c1.013,0,1.846-0.073,2.5-0.22c0.652-0.146,1.246-0.313,1.779-0.5l1.08,3.96c-0.934,0.373-1.854,0.641-2.76,0.801
				s-1.92,0.239-3.04,0.239c-1.866,0-3.474-0.26-4.819-0.78c-1.348-0.52-2.461-1.227-3.341-2.119
				c-0.88-0.895-1.534-1.94-1.96-3.141c-0.427-1.199-0.641-2.48-0.641-3.84C43.081,128.282,43.288,126.989,43.702,125.789z"></path>
			<path class="clc-pattern-unit-letter" d="M79.362,125.028c-0.215-0.079-0.654-0.253-1.32-0.52c-0.667-0.267-1.494-0.4-2.48-0.4c-0.773,0-1.414,0.227-1.92,0.681
				c-0.507,0.453-0.76,1.319-0.76,2.601v20.399h-10.12v-4.08h5.2v-15.96c0-2.587,0.52-4.554,1.561-5.9
				c1.039-1.347,2.692-2.02,4.959-2.02c0.826,0,1.566,0.054,2.221,0.16c0.652,0.106,1.206,0.233,1.66,0.38
				c0.453,0.146,0.819,0.286,1.1,0.42c0.279,0.134,0.46,0.226,0.54,0.28L79.362,125.028z"></path>
			<path class="clc-pattern-unit-letter" d="M83.702,125.789c0.412-1.2,1.06-2.24,1.939-3.12c0.881-0.88,2.006-1.574,3.381-2.08c1.373-0.506,3.02-0.76,4.939-0.76
				c1.334,0,2.494,0.087,3.48,0.26c0.986,0.174,1.826,0.393,2.52,0.66l-0.721,4.04c-0.693-0.24-1.459-0.413-2.299-0.521
				c-0.84-0.106-1.646-0.16-2.42-0.16c-2.348,0-4.008,0.494-4.98,1.48c-0.975,0.986-1.461,2.346-1.461,4.08
				c0,0.801,0.107,1.547,0.32,2.24s0.561,1.286,1.041,1.779c0.48,0.494,1.113,0.881,1.9,1.16c0.785,0.28,1.766,0.42,2.939,0.42
				c1.014,0,1.846-0.073,2.5-0.22c0.652-0.146,1.246-0.313,1.779-0.5l1.08,3.96c-0.934,0.373-1.854,0.641-2.76,0.801
				s-1.92,0.239-3.039,0.239c-1.867,0-3.475-0.26-4.82-0.78c-1.348-0.52-2.461-1.227-3.341-2.119
				c-0.88-0.895-1.534-1.94-1.96-3.141c-0.427-1.199-0.641-2.48-0.641-3.84C83.081,128.282,83.288,126.989,83.702,125.789z"></path>
			<path class="clc-pattern-unit-letter" d="M201.856,129.668c0,1.359-0.215,2.641-0.641,3.84c-0.426,1.2-1.08,2.246-1.961,3.141c-0.879,0.893-1.992,1.6-3.34,2.119
				c-1.346,0.521-2.953,0.78-4.82,0.78c-1.119,0-2.133-0.079-3.039-0.239s-1.826-0.428-2.76-0.801l1.08-3.96
				c0.533,0.187,1.127,0.354,1.779,0.5c0.654,0.146,1.486,0.22,2.5,0.22c1.174,0,2.154-0.14,2.939-0.42
				c0.787-0.279,1.42-0.666,1.9-1.16c0.48-0.493,0.828-1.086,1.041-1.779s0.32-1.439,0.32-2.24c0-1.734-0.486-3.094-1.461-4.08
				c-0.973-0.986-2.633-1.48-4.98-1.48c-0.773,0-1.58,0.054-2.42,0.16c-0.84,0.107-1.605,0.28-2.299,0.521l-0.721-4.04
				c0.693-0.268,1.533-0.486,2.52-0.66c0.986-0.173,2.146-0.26,3.48-0.26c1.92,0,3.566,0.254,4.939,0.76
				c1.375,0.506,2.5,1.2,3.381,2.08c0.879,0.88,1.527,1.92,1.939,3.12C201.649,126.989,201.856,128.282,201.856,129.668z"></path>
			<path class="clc-pattern-unit-letter" d="M164.935,121.069c0.08-0.055,0.262-0.146,0.541-0.28s0.646-0.274,1.1-0.42c0.453-0.146,1.008-0.273,1.66-0.38
				c0.654-0.106,1.395-0.16,2.221-0.16c2.266,0,3.92,0.673,4.959,2.02c1.041,1.347,1.561,3.313,1.561,5.9v15.96h5.199v4.08h-10.119
				v-20.399c0-1.281-0.254-2.147-0.76-2.601c-0.506-0.454-1.146-0.681-1.92-0.681c-0.986,0-1.814,0.134-2.48,0.4
				s-1.105,0.44-1.32,0.52L164.935,121.069z"></path>
			<path class="clc-pattern-unit-letter" d="M161.856,129.668c0,1.359-0.215,2.641-0.641,3.84c-0.426,1.2-1.08,2.246-1.961,3.141c-0.879,0.893-1.992,1.6-3.34,2.119
				c-1.346,0.521-2.953,0.78-4.82,0.78c-1.119,0-2.133-0.079-3.039-0.239s-1.826-0.428-2.76-0.801l1.08-3.96
				c0.533,0.187,1.127,0.354,1.779,0.5c0.654,0.146,1.486,0.22,2.5,0.22c1.174,0,2.154-0.14,2.939-0.42
				c0.787-0.279,1.42-0.666,1.9-1.16c0.48-0.493,0.828-1.086,1.041-1.779s0.32-1.439,0.32-2.24c0-1.734-0.486-3.094-1.461-4.08
				c-0.973-0.986-2.633-1.48-4.98-1.48c-0.773,0-1.58,0.054-2.42,0.16c-0.84,0.107-1.605,0.28-2.299,0.521l-0.721-4.04
				c0.693-0.268,1.533-0.486,2.52-0.66c0.986-0.173,2.146-0.26,3.48-0.26c1.92,0,3.566,0.254,4.939,0.76
				c1.375,0.506,2.5,1.2,3.381,2.08c0.879,0.88,1.527,1.92,1.939,3.12C161.649,126.989,161.856,128.282,161.856,129.668z"></path>
			<path class="clc-pattern-unit-letter" d="M124.935,121.069c0.08-0.055,0.262-0.146,0.541-0.28s0.646-0.274,1.1-0.42c0.453-0.146,1.008-0.273,1.66-0.38
				c0.654-0.106,1.395-0.16,2.221-0.16c2.266,0,3.92,0.673,4.959,2.02c1.041,1.347,1.561,3.313,1.561,5.9v15.96h5.199v4.08h-10.119
				v-20.399c0-1.281-0.254-2.147-0.76-2.601c-0.506-0.454-1.146-0.681-1.92-0.681c-0.986,0-1.814,0.134-2.48,0.4
				s-1.105,0.44-1.32,0.52L124.935,121.069z"></path>
			<path class="clc-pattern-unit-letter" d="M121.856,129.668c0,1.359-0.215,2.641-0.641,3.84c-0.426,1.2-1.08,2.246-1.961,3.141c-0.879,0.893-1.992,1.6-3.34,2.119
				c-1.346,0.521-2.953,0.78-4.82,0.78c-1.119,0-2.133-0.079-3.039-0.239s-1.826-0.428-2.76-0.801l1.08-3.96
				c0.533,0.187,1.127,0.354,1.779,0.5c0.654,0.146,1.486,0.22,2.5,0.22c1.174,0,2.154-0.14,2.939-0.42
				c0.787-0.279,1.42-0.666,1.9-1.16c0.48-0.493,0.828-1.086,1.041-1.779s0.32-1.439,0.32-2.24c0-1.734-0.486-3.094-1.461-4.08
				c-0.973-0.986-2.633-1.48-4.98-1.48c-0.773,0-1.58,0.054-2.42,0.16c-0.84,0.107-1.605,0.28-2.299,0.521l-0.721-4.04
				c0.693-0.268,1.533-0.486,2.52-0.66c0.986-0.173,2.146-0.26,3.48-0.26c1.92,0,3.566,0.254,4.939,0.76
				c1.375,0.506,2.5,1.2,3.381,2.08c0.879,0.88,1.527,1.92,1.939,3.12C121.649,126.989,121.856,128.282,121.856,129.668z"></path>
			<path class="clc-pattern-unit-letter" d="M19.362,86.158c-0.215-0.079-0.654-0.253-1.32-0.52c-0.667-0.267-1.494-0.4-2.48-0.4c-0.773,0-1.414,0.227-1.92,0.681
				c-0.507,0.453-0.76,1.319-0.76,2.601v20.399H2.762v-4.08h5.2v-15.96c0-2.587,0.52-4.554,1.561-5.9
				c1.039-1.347,2.692-2.02,4.959-2.02c0.826,0,1.566,0.054,2.221,0.16c0.652,0.106,1.206,0.233,1.66,0.38
				c0.453,0.146,0.819,0.286,1.1,0.42c0.279,0.134,0.46,0.226,0.54,0.28L19.362,86.158z"></path>
			<path class="clc-pattern-unit-letter" d="M23.702,86.918c0.412-1.2,1.06-2.24,1.939-3.12c0.881-0.88,2.006-1.574,3.381-2.08c1.373-0.506,3.02-0.76,4.939-0.76
				c1.333,0,2.493,0.087,3.48,0.26c0.986,0.174,1.826,0.393,2.52,0.66l-0.721,4.04c-0.693-0.24-1.459-0.413-2.299-0.521
				c-0.841-0.106-1.647-0.16-2.42-0.16c-2.348,0-4.008,0.494-4.98,1.48c-0.975,0.986-1.461,2.346-1.461,4.08
				c0,0.801,0.107,1.547,0.32,2.24s0.561,1.286,1.041,1.779c0.479,0.494,1.112,0.881,1.899,1.16c0.786,0.28,1.767,0.42,2.94,0.42
				c1.013,0,1.846-0.073,2.5-0.22c0.652-0.146,1.246-0.313,1.779-0.5l1.08,3.96c-0.934,0.373-1.854,0.641-2.76,0.801
				s-1.92,0.239-3.04,0.239c-1.866,0-3.474-0.26-4.819-0.78c-1.348-0.52-2.461-1.227-3.341-2.119
				c-0.88-0.895-1.534-1.94-1.96-3.141c-0.427-1.199-0.641-2.48-0.641-3.84C23.081,89.412,23.288,88.119,23.702,86.918z"></path>
			<path class="clc-pattern-unit-letter" d="M59.362,86.158c-0.215-0.079-0.654-0.253-1.32-0.52c-0.667-0.267-1.494-0.4-2.48-0.4c-0.773,0-1.414,0.227-1.92,0.681
				c-0.507,0.453-0.76,1.319-0.76,2.601v20.399h-10.12v-4.08h5.2v-15.96c0-2.587,0.52-4.554,1.561-5.9
				c1.039-1.347,2.692-2.02,4.959-2.02c0.826,0,1.566,0.054,2.221,0.16c0.652,0.106,1.206,0.233,1.66,0.38
				c0.453,0.146,0.819,0.286,1.1,0.42c0.279,0.134,0.46,0.226,0.54,0.28L59.362,86.158z"></path>
			<path class="clc-pattern-unit-letter" d="M63.702,86.918c0.412-1.2,1.06-2.24,1.939-3.12c0.881-0.88,2.006-1.574,3.381-2.08c1.373-0.506,3.02-0.76,4.939-0.76
				c1.333,0,2.493,0.087,3.48,0.26c0.986,0.174,1.826,0.393,2.52,0.66l-0.721,4.04c-0.693-0.24-1.459-0.413-2.299-0.521
				c-0.841-0.106-1.647-0.16-2.42-0.16c-2.348,0-4.008,0.494-4.98,1.48c-0.975,0.986-1.461,2.346-1.461,4.08
				c0,0.801,0.107,1.547,0.32,2.24s0.561,1.286,1.041,1.779c0.479,0.494,1.112,0.881,1.899,1.16c0.786,0.28,1.767,0.42,2.94,0.42
				c1.013,0,1.846-0.073,2.5-0.22c0.652-0.146,1.246-0.313,1.779-0.5l1.08,3.96c-0.934,0.373-1.854,0.641-2.76,0.801
				s-1.92,0.239-3.04,0.239c-1.866,0-3.474-0.26-4.819-0.78c-1.348-0.52-2.461-1.227-3.341-2.119
				c-0.88-0.895-1.534-1.94-1.96-3.141c-0.427-1.199-0.641-2.48-0.641-3.84C63.081,89.412,63.288,88.119,63.702,86.918z"></path>
			<path class="clc-pattern-unit-letter" d="M99.362,86.158c-0.215-0.079-0.654-0.253-1.32-0.52s-1.494-0.4-2.48-0.4c-0.773,0-1.414,0.227-1.92,0.681
				c-0.506,0.453-0.76,1.319-0.76,2.601v20.399h-10.12v-4.08h5.2v-15.96c0-2.587,0.52-4.554,1.561-5.9
				c1.039-1.347,2.693-2.02,4.959-2.02c0.826,0,1.566,0.054,2.221,0.16c0.652,0.106,1.207,0.233,1.66,0.38
				c0.453,0.146,0.82,0.286,1.1,0.42s0.461,0.226,0.541,0.28L99.362,86.158z"></path>
			<path class="clc-pattern-unit-letter" d="M184.935,82.199c0.08-0.055,0.262-0.146,0.541-0.28s0.646-0.274,1.1-0.42c0.453-0.146,1.008-0.273,1.66-0.38
				c0.654-0.106,1.395-0.16,2.221-0.16c2.266,0,3.92,0.673,4.959,2.02c1.041,1.347,1.561,3.313,1.561,5.9v15.96h5.199v4.08h-10.119
				V88.519c0-1.281-0.254-2.147-0.76-2.601c-0.506-0.454-1.146-0.681-1.92-0.681c-0.986,0-1.814,0.134-2.48,0.4
				s-1.105,0.44-1.32,0.52L184.935,82.199z"></path>
			<path class="clc-pattern-unit-letter" d="M181.856,90.798c0,1.359-0.215,2.641-0.641,3.84c-0.426,1.2-1.08,2.246-1.961,3.141c-0.879,0.893-1.992,1.6-3.34,2.119
				c-1.346,0.521-2.953,0.78-4.82,0.78c-1.119,0-2.133-0.079-3.039-0.239s-1.826-0.428-2.76-0.801l1.08-3.96
				c0.533,0.187,1.127,0.354,1.779,0.5c0.654,0.146,1.486,0.22,2.5,0.22c1.174,0,2.154-0.14,2.939-0.42
				c0.787-0.279,1.42-0.666,1.9-1.16c0.48-0.493,0.828-1.086,1.041-1.779s0.32-1.439,0.32-2.24c0-1.734-0.486-3.094-1.461-4.08
				c-0.973-0.986-2.633-1.48-4.98-1.48c-0.773,0-1.58,0.054-2.42,0.16c-0.84,0.107-1.605,0.28-2.299,0.521l-0.721-4.04
				c0.693-0.268,1.533-0.486,2.52-0.66c0.986-0.173,2.146-0.26,3.48-0.26c1.92,0,3.566,0.254,4.939,0.76
				c1.375,0.506,2.5,1.2,3.381,2.08c0.879,0.88,1.527,1.92,1.939,3.12C181.649,88.119,181.856,89.412,181.856,90.798z"></path>
			<path class="clc-pattern-unit-letter" d="M144.935,82.199c0.08-0.055,0.262-0.146,0.541-0.28s0.646-0.274,1.1-0.42c0.453-0.146,1.008-0.273,1.66-0.38
				c0.654-0.106,1.395-0.16,2.221-0.16c2.266,0,3.92,0.673,4.959,2.02c1.041,1.347,1.561,3.313,1.561,5.9v15.96h5.199v4.08h-10.119
				V88.519c0-1.281-0.254-2.147-0.76-2.601c-0.506-0.454-1.146-0.681-1.92-0.681c-0.986,0-1.814,0.134-2.48,0.4
				s-1.105,0.44-1.32,0.52L144.935,82.199z"></path>
			<path class="clc-pattern-unit-letter" d="M141.856,90.798c0,1.359-0.215,2.641-0.641,3.84c-0.426,1.2-1.08,2.246-1.961,3.141c-0.879,0.893-1.992,1.6-3.34,2.119
				c-1.346,0.521-2.953,0.78-4.82,0.78c-1.119,0-2.133-0.079-3.039-0.239s-1.826-0.428-2.76-0.801l1.08-3.96
				c0.533,0.187,1.127,0.354,1.779,0.5c0.654,0.146,1.486,0.22,2.5,0.22c1.174,0,2.154-0.14,2.939-0.42
				c0.787-0.279,1.42-0.666,1.9-1.16c0.48-0.493,0.828-1.086,1.041-1.779s0.32-1.439,0.32-2.24c0-1.734-0.486-3.094-1.461-4.08
				c-0.973-0.986-2.633-1.48-4.98-1.48c-0.773,0-1.58,0.054-2.42,0.16c-0.84,0.107-1.605,0.28-2.299,0.521l-0.721-4.04
				c0.693-0.268,1.533-0.486,2.52-0.66c0.986-0.173,2.146-0.26,3.48-0.26c1.92,0,3.566,0.254,4.939,0.76
				c1.375,0.506,2.5,1.2,3.381,2.08c0.879,0.88,1.527,1.92,1.939,3.12C141.649,88.119,141.856,89.412,141.856,90.798z"></path>
			<path class="clc-pattern-unit-letter" d="M104.935,82.199c0.08-0.055,0.262-0.146,0.541-0.28s0.646-0.274,1.1-0.42c0.453-0.146,1.008-0.273,1.66-0.38
				c0.654-0.106,1.395-0.16,2.221-0.16c2.266,0,3.92,0.673,4.959,2.02c1.041,1.347,1.561,3.313,1.561,5.9v15.96h5.199v4.08h-10.119
				V88.519c0-1.281-0.254-2.147-0.76-2.601c-0.506-0.454-1.146-0.681-1.92-0.681c-0.986,0-1.814,0.134-2.48,0.4
				s-1.105,0.44-1.32,0.52L104.935,82.199z"></path>
			<path class="clc-pattern-unit-letter" d="M19.362,86.158c-0.215-0.079-0.654-0.253-1.32-0.52c-0.667-0.267-1.494-0.4-2.48-0.4c-0.773,0-1.414,0.227-1.92,0.681
				c-0.507,0.453-0.76,1.319-0.76,2.601v20.399H2.762v-4.08h5.2v-15.96c0-2.587,0.52-4.554,1.561-5.9
				c1.039-1.347,2.692-2.02,4.959-2.02c0.826,0,1.566,0.054,2.221,0.16c0.652,0.106,1.206,0.233,1.66,0.38
				c0.453,0.146,0.819,0.286,1.1,0.42c0.279,0.134,0.46,0.226,0.54,0.28L19.362,86.158z"></path>
			<path class="clc-pattern-unit-letter" d="M23.702,86.918c0.412-1.2,1.06-2.24,1.939-3.12c0.881-0.88,2.006-1.574,3.381-2.08c1.373-0.506,3.02-0.76,4.939-0.76
				c1.333,0,2.493,0.087,3.48,0.26c0.986,0.174,1.826,0.393,2.52,0.66l-0.721,4.04c-0.693-0.24-1.459-0.413-2.299-0.521
				c-0.841-0.106-1.647-0.16-2.42-0.16c-2.348,0-4.008,0.494-4.98,1.48c-0.975,0.986-1.461,2.346-1.461,4.08
				c0,0.801,0.107,1.547,0.32,2.24s0.561,1.286,1.041,1.779c0.479,0.494,1.112,0.881,1.899,1.16c0.786,0.28,1.767,0.42,2.94,0.42
				c1.013,0,1.846-0.073,2.5-0.22c0.652-0.146,1.246-0.313,1.779-0.5l1.08,3.96c-0.934,0.373-1.854,0.641-2.76,0.801
				s-1.92,0.239-3.04,0.239c-1.866,0-3.474-0.26-4.819-0.78c-1.348-0.52-2.461-1.227-3.341-2.119
				c-0.88-0.895-1.534-1.94-1.96-3.141c-0.427-1.199-0.641-2.48-0.641-3.84C23.081,89.412,23.288,88.119,23.702,86.918z"></path>
			<path class="clc-pattern-unit-letter" d="M59.362,86.158c-0.215-0.079-0.654-0.253-1.32-0.52c-0.667-0.267-1.494-0.4-2.48-0.4c-0.773,0-1.414,0.227-1.92,0.681
				c-0.507,0.453-0.76,1.319-0.76,2.601v20.399h-10.12v-4.08h5.2v-15.96c0-2.587,0.52-4.554,1.561-5.9
				c1.039-1.347,2.692-2.02,4.959-2.02c0.826,0,1.566,0.054,2.221,0.16c0.652,0.106,1.206,0.233,1.66,0.38
				c0.453,0.146,0.819,0.286,1.1,0.42c0.279,0.134,0.46,0.226,0.54,0.28L59.362,86.158z"></path>
			<path class="clc-pattern-unit-letter" d="M63.702,86.918c0.412-1.2,1.06-2.24,1.939-3.12c0.881-0.88,2.006-1.574,3.381-2.08c1.373-0.506,3.02-0.76,4.939-0.76
				c1.333,0,2.493,0.087,3.48,0.26c0.986,0.174,1.826,0.393,2.52,0.66l-0.721,4.04c-0.693-0.24-1.459-0.413-2.299-0.521
				c-0.841-0.106-1.647-0.16-2.42-0.16c-2.348,0-4.008,0.494-4.98,1.48c-0.975,0.986-1.461,2.346-1.461,4.08
				c0,0.801,0.107,1.547,0.32,2.24s0.561,1.286,1.041,1.779c0.479,0.494,1.112,0.881,1.899,1.16c0.786,0.28,1.767,0.42,2.94,0.42
				c1.013,0,1.846-0.073,2.5-0.22c0.652-0.146,1.246-0.313,1.779-0.5l1.08,3.96c-0.934,0.373-1.854,0.641-2.76,0.801
				s-1.92,0.239-3.04,0.239c-1.866,0-3.474-0.26-4.819-0.78c-1.348-0.52-2.461-1.227-3.341-2.119
				c-0.88-0.895-1.534-1.94-1.96-3.141c-0.427-1.199-0.641-2.48-0.641-3.84C63.081,89.412,63.288,88.119,63.702,86.918z"></path>
			<path class="clc-pattern-unit-letter" d="M99.362,86.158c-0.215-0.079-0.654-0.253-1.32-0.52s-1.494-0.4-2.48-0.4c-0.773,0-1.414,0.227-1.92,0.681
				c-0.506,0.453-0.76,1.319-0.76,2.601v20.399h-10.12v-4.08h5.2v-15.96c0-2.587,0.52-4.554,1.561-5.9
				c1.039-1.347,2.693-2.02,4.959-2.02c0.826,0,1.566,0.054,2.221,0.16c0.652,0.106,1.207,0.233,1.66,0.38
				c0.453,0.146,0.82,0.286,1.1,0.42s0.461,0.226,0.541,0.28L99.362,86.158z"></path>
			<path class="clc-pattern-unit-letter" d="M184.935,82.199c0.08-0.055,0.262-0.146,0.541-0.28s0.646-0.274,1.1-0.42c0.453-0.146,1.008-0.273,1.66-0.38
				c0.654-0.106,1.395-0.16,2.221-0.16c2.266,0,3.92,0.673,4.959,2.02c1.041,1.347,1.561,3.313,1.561,5.9v15.96h5.199v4.08h-10.119
				V88.519c0-1.281-0.254-2.147-0.76-2.601c-0.506-0.454-1.146-0.681-1.92-0.681c-0.986,0-1.814,0.134-2.48,0.4
				s-1.105,0.44-1.32,0.52L184.935,82.199z"></path>
			<path class="clc-pattern-unit-letter" d="M181.856,90.798c0,1.359-0.215,2.641-0.641,3.84c-0.426,1.2-1.08,2.246-1.961,3.141c-0.879,0.893-1.992,1.6-3.34,2.119
				c-1.346,0.521-2.953,0.78-4.82,0.78c-1.119,0-2.133-0.079-3.039-0.239s-1.826-0.428-2.76-0.801l1.08-3.96
				c0.533,0.187,1.127,0.354,1.779,0.5c0.654,0.146,1.486,0.22,2.5,0.22c1.174,0,2.154-0.14,2.939-0.42
				c0.787-0.279,1.42-0.666,1.9-1.16c0.48-0.493,0.828-1.086,1.041-1.779s0.32-1.439,0.32-2.24c0-1.734-0.486-3.094-1.461-4.08
				c-0.973-0.986-2.633-1.48-4.98-1.48c-0.773,0-1.58,0.054-2.42,0.16c-0.84,0.107-1.605,0.28-2.299,0.521l-0.721-4.04
				c0.693-0.268,1.533-0.486,2.52-0.66c0.986-0.173,2.146-0.26,3.48-0.26c1.92,0,3.566,0.254,4.939,0.76
				c1.375,0.506,2.5,1.2,3.381,2.08c0.879,0.88,1.527,1.92,1.939,3.12C181.649,88.119,181.856,89.412,181.856,90.798z"></path>
			<path class="clc-pattern-unit-letter" d="M144.935,82.199c0.08-0.055,0.262-0.146,0.541-0.28s0.646-0.274,1.1-0.42c0.453-0.146,1.008-0.273,1.66-0.38
				c0.654-0.106,1.395-0.16,2.221-0.16c2.266,0,3.92,0.673,4.959,2.02c1.041,1.347,1.561,3.313,1.561,5.9v15.96h5.199v4.08h-10.119
				V88.519c0-1.281-0.254-2.147-0.76-2.601c-0.506-0.454-1.146-0.681-1.92-0.681c-0.986,0-1.814,0.134-2.48,0.4
				s-1.105,0.44-1.32,0.52L144.935,82.199z"></path>
			<path class="clc-pattern-unit-letter" d="M141.856,90.798c0,1.359-0.215,2.641-0.641,3.84c-0.426,1.2-1.08,2.246-1.961,3.141c-0.879,0.893-1.992,1.6-3.34,2.119
				c-1.346,0.521-2.953,0.78-4.82,0.78c-1.119,0-2.133-0.079-3.039-0.239s-1.826-0.428-2.76-0.801l1.08-3.96
				c0.533,0.187,1.127,0.354,1.779,0.5c0.654,0.146,1.486,0.22,2.5,0.22c1.174,0,2.154-0.14,2.939-0.42
				c0.787-0.279,1.42-0.666,1.9-1.16c0.48-0.493,0.828-1.086,1.041-1.779s0.32-1.439,0.32-2.24c0-1.734-0.486-3.094-1.461-4.08
				c-0.973-0.986-2.633-1.48-4.98-1.48c-0.773,0-1.58,0.054-2.42,0.16c-0.84,0.107-1.605,0.28-2.299,0.521l-0.721-4.04
				c0.693-0.268,1.533-0.486,2.52-0.66c0.986-0.173,2.146-0.26,3.48-0.26c1.92,0,3.566,0.254,4.939,0.76
				c1.375,0.506,2.5,1.2,3.381,2.08c0.879,0.88,1.527,1.92,1.939,3.12C141.649,88.119,141.856,89.412,141.856,90.798z"></path>
			<path class="clc-pattern-unit-letter" d="M104.935,82.199c0.08-0.055,0.262-0.146,0.541-0.28s0.646-0.274,1.1-0.42c0.453-0.146,1.008-0.273,1.66-0.38
				c0.654-0.106,1.395-0.16,2.221-0.16c2.266,0,3.92,0.673,4.959,2.02c1.041,1.347,1.561,3.313,1.561,5.9v15.96h5.199v4.08h-10.119
					V88.519c0-1.281-0.254-2.147-0.76-2.601c-0.506-0.454-1.146-0.681-1.92-0.681c-0.986,0-1.814,0.134-2.48,0.4
					s-1.105,0.44-1.32,0.52L104.935,82.199z"></path>
		</pattern>
	</defs>

	<rect x="0" y="0" width="2000" height="2000" fill="url(#clc-pattern-unit)"></rect>
	<!-- transform="scale(0.488064)" -->

</svg>

			</div>
		</div>
	</div>


<div class="container pt-5">
  <div class="row">
    <div class="col-lg-3">
                <nav id="TOC" role="doc-toc" class="sticky-top small pt-5">
          <h4>Contents</h4>
        <ul class="incremental">
        <li><a href="#feed-forward-neural-networks"><span class="toc-section-number">1</span> Feed Forward Neural Networks</a><ul class="incremental">
        <li><a href="#data"><span class="toc-section-number">1.1</span> Data</a></li>
        <li><a href="#architecture"><span class="toc-section-number">1.2</span> Architecture</a></li>
        <li><a href="#training"><span class="toc-section-number">1.3</span> Training</a></li>
        </ul></li>
        <li><a href="#word-embeddings"><span class="toc-section-number">2</span> Word Embeddings</a></li>
        <li><a href="#recurrent-neural-networks"><span class="toc-section-number">3</span> Recurrent Neural Networks</a></li>
        </ul>
        </nav>
            </div>
    <div class="col-lg-9 content">
<p><strong>Goals.</strong> In this computer lab, you become familiar with implementations of some common neural network models: the feed forward neural network, and the recurrent neural network. We will apply these networks to natural language modelling.</p>
<p><strong>Requirements.</strong> This assignment uses Python 3 and the library <code>numpy</code>, an extremely useful and popular library for working with vectors and matrices in Python. <a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html">Here you can find a quick introduction to Numpy</a>. You also need to install the python libraries <code>nltk</code> (the <a href="http://www.nltk.org/">Natural Language Toolkit</a>) and <code>sklearn</code> (Scikit-learn). You can install both libraries easily with <code>pip</code> or <code>conda</code>.</p>
<p><strong>Note on training.</strong> It may take long for the models in this assignment to be trained, if it is unreasonably slow on your computers it’s ok to run them for fewer iterations. But of course this means you will get worse results and they will be harder to interpret.</p>
<h1 id="feed-forward-neural-networks"><span class="header-section-number">1</span> Feed Forward Neural Networks</h1>
<p>Let’s jump right in. In the first part of the assignment, you will train a feed forward neural network to predict the next word, given the current word. In other words, you train it to generate bigrams. The idea is that we feed a word to the network and want it to output the next word. To get the network to do this, we need to <em>train</em> it. The training data will be a collection bigrams taken from some corpus. During training, we feed the first word of a bigram (the <em>input</em>) to the network, and see if the output matches the second word of the bigram (the <em>target</em>). Next we update the parameters of the network in such a way that predicting the target becomes more likely.</p>
<h2 id="data"><span class="header-section-number">1.1</span> Data</h2>
<p>Before we turn to the network, we need some training data. The file <code>DataReader.py</code> contains a class <code>DataReader</code> that reads out a text file, extracts (preprocessed) bigrams which serve as the training data. In this lab we will use Reddit comments as training data, which can be found in <code>materials/reddit-comments-2015-08.txt</code>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">from</span> DataReader <span class="im">import</span> DataReader</a>
<a class="sourceLine" id="cb1-2" data-line-number="2">dr <span class="op">=</span> DataReader(<span class="st">&#39;reddit-comments-2015-08.txt&#39;</span>)</a>
<a class="sourceLine" id="cb1-3" data-line-number="3">inputs, targets <span class="op">=</span> dr.get_training_bigrams()</a></code></pre></div>
<p>If you run this code, you might get the error <code>&quot;Resource punkt not found&quot;</code> because the <code>nltk</code> library uses datasets that you have to download first. The code should run after following the instructions in the error message:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb2-1" data-line-number="1">$ <span class="ex">python</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2"><span class="op">&gt;&gt;&gt;</span> <span class="ex">import</span> nltk</a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="op">&gt;&gt;&gt;</span> <span class="ex">nltk.download</span>(<span class="st">&#39;punkt&#39;</span>)</a></code></pre></div>
<p>You can see that <code>inputs</code> and <code>targets</code> are large matrices filled with zeros and ones. Why is that, if they should be words and the next words? Because we can’t feed actual words (text) to a neural network: we have to represent them as numerical vectors. The simplest representation is a so called <em>one-hot encoding</em>. If we have vocabulary of <span class="math inline">\(M\)</span> words, the one-hot representation of the <span class="math inline">\(i\)</span>-th word is a vector of length <span class="math inline">\(M\)</span> with only zeros, except for a 1 at at position <span class="math inline">\(i\)</span>.</p>
<p>The <code>DataReader</code> implements all this: it computes training bigrams in a one-hot representation. You can en/decode words using the <code>dr.index_to_word</code> and <code>dr.word_to_index</code> dictionaries. For example, you can find the the 4-th input word using <code>dr.index_to_word[inputs[4].argmax()]</code>.</p>
<h2 id="architecture"><span class="header-section-number">1.2</span> Architecture</h2>
<p>If you are not familiar with artificial neural networks, do look for some introductions. Very briefly, a network consists of nodes organized in layers, such as an input layer, a hidden layer and an output layer. Every node has an activation (a number), which is computed from the activations of the nodes feeding into it, and the weights of the corresponding connections. Mathematically, the activation of a node is the weighted sum of the incoming activations – nearly, since we further apply a non-linearity such as <span class="math inline">\(tanh\)</span>. It turns out that we can conveniently compute all activations in a layer at once using matrix multiplication. To give you an idea: if <span class="math inline">\(x = (x_1, ..., x_n)\)</span> is a vector with the activations of the input nodes, and <span class="math inline">\(W\)</span> is a <span class="math inline">\(m \times n\)</span> matrix containing the weights of all connections between this and the next layer, then the activations <span class="math inline">\(y = (y_1, ..., y_m)\)</span> of the next layer are <span class="math inline">\(y = \tanh(W . x)\)</span>, where ‘.’ is the <a href="https://en.wikipedia.org/wiki/Dot_product">dot-product</a>.</p>
<p>The neural network we focus one here has one hidden layer, and therefore two weight matrices: <span class="math inline">\(W_{in}\)</span>, which maps the input layer to the hidden layer; and <span class="math inline">\(W_{out}\)</span>, which maps the hidden layer to the output layer. The weights are the trainable parameters of the network. In the case of next word prediction, both the input and output layers represent words and must have the same size (namely, the vocabulary size). The hidden layer, however, can have any dimensionality. Which size of the hidden layer works best is depends on the task. It is a so called <em>hyper-parameter</em> – more about that later.</p>
<p>In <code>FeedForward.py</code> you find the <code>FeedForwardNN</code> class which implements a feed forward neural network with one hidden layer. Try to understand how the codes work by looking at the methods of the class and reading the comments if it’s not clear what each method is doing.</p>
<div class="question">
<p>The <code>init_params</code> method initializes the weight matrices <span class="math inline">\(W_{in}\)</span> and <span class="math inline">\(W_{out}\)</span>.</p>
<ul class="incremental">
<li><p>What should be the dimensions of these matrices?</p></li>
<li><p>What is the total number of parameters of this model?</p></li>
<li>You already see the code that initializes <span class="math inline">\(W_{in}\)</span>. Adjust it to also initialize <span class="math inline">\(W_{out}\)</span>.</li>
</ul>
</div>
<p>In a feed-forward neural network forward propagation is passing the input signal through the network while multiplying it by the respective weights to compute an output. The <code>forward_pass</code> method implements all these computations for you: it computes the output of the network for a given input and model parameters.</p>
<div class="question">
<p>Look at the <code>forward_pass</code> method and draw a graph that illustrates how the output of the network is computed based on the input and model parameters. Include the relevant mathematical operations.</p>
</div>
<p>Here is how you can instantiate <code>FeedForwardNN</code>, and apply it to the data.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1">neural_network <span class="op">=</span> FeedForwardNN(input_dim, hidden_dim, output_dim)</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">hidden_state, output_state <span class="op">=</span> neural_network.forward_pass(inputs)</a></code></pre></div>
<div class="question">
<p>What is the loss of the model before training? Compute this using the <code>calculate_loss</code> method.</p>
</div>
<h2 id="training"><span class="header-section-number">1.3</span> Training</h2>
<p>Now, let’s train the model. There are all kinds of search algorithms to find the optimal parameters of a model: grid search and hill climbing are two examples of such optimization algorithms. But it would be very inefficient to use something like grid search to optimize the parameters of a neural network, because it can have such a large number of parameters. Instead, neural networks are commonly trained using <a href="https://en.wikipedia.org/wiki/Backpropagation"><em>back propagation</em></a> (backward propagation of errors) along with an optimization method such as <a href="https://en.wikipedia.org/wiki/Gradient_descent"><em>gradient descent</em></a>. The idea behind gradient descent is to change the parameters in a direction that decreases the loss. But instead of examining neighbouring points randomly like in hill climbing, the gradient of the loss function is used to determine the direction of the change. We won’t explain back propagation in detail here. If you are interested, look at this blogposts by <a href="http://colah.github.io/posts/2015-08-Backprop">Chris Olah</a>, or <a href="http://neuralnetworksanddeeplearning.com/chap2.html">this chapter by Michael Nielsen</a>.</p>
<p>There are three variants of back-propagation:</p>
<ul class="incremental">
<li><strong>batch training:</strong> all the training items are used to calculate an accumulated gradient for each weight and bias, and then each weight value is adjusted.</li>
<li><strong>online training:</strong> the gradients are calculated for each individual training item, and then each weight value is adjusted using the estimated gradients.</li>
<li><strong>mini-batch training:</strong> a batch of training items is used to compute the estimated gradients, and then each weight value is adjusted using the estimated gradients.</li>
</ul>
<p>You can consider the choice of one of these variants of backpropagation a parameter of the training procedure. In order to distinguish the training parameters from the model parameters, we refer to the former as <em>hyperparameters</em>. Other hyperparameters are the learning rate (how much we adjust the weights in each iteration); the number of <em>epochs</em> or training iterations and architectural choices like the size of the hidden layer.</p>
<p>The <code>back_propagate_update</code> method in the <code>FeedForwardNN</code> class contains the code for updating parameters of the network with backpropagation. This method is called in the <code>train</code> method, which takes the hyperparameters as optional arguments. It returns an array with the loss after every iteration. The following code demonstrates how you can train the network:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="co"># Load the training data; inputs = words, outputs = next words</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2">dr <span class="op">=</span> DataReader()</a>
<a class="sourceLine" id="cb4-3" data-line-number="3">inputs, targets <span class="op">=</span> dr.get_trainig_bigrams()</a>
<a class="sourceLine" id="cb4-4" data-line-number="4"></a>
<a class="sourceLine" id="cb4-5" data-line-number="5"><span class="co"># Initialize the neural network</span></a>
<a class="sourceLine" id="cb4-6" data-line-number="6">vocabulary_size <span class="op">=</span> <span class="bu">len</span>(inputs[<span class="dv">0</span>]) <span class="co"># = length of the first word-vector</span></a>
<a class="sourceLine" id="cb4-7" data-line-number="7">neural_network <span class="op">=</span> FeedForwardNN(</a>
<a class="sourceLine" id="cb4-8" data-line-number="8">    input_dim<span class="op">=</span>vocabulary_size,</a>
<a class="sourceLine" id="cb4-9" data-line-number="9">    hidden_dim<span class="op">=</span><span class="dv">256</span>,</a>
<a class="sourceLine" id="cb4-10" data-line-number="10">    output_dim<span class="op">=</span>vocabulary_size)</a>
<a class="sourceLine" id="cb4-11" data-line-number="11"></a>
<a class="sourceLine" id="cb4-12" data-line-number="12"><span class="co"># Train the network</span></a>
<a class="sourceLine" id="cb4-13" data-line-number="13">trace <span class="op">=</span> neural_network.train(</a>
<a class="sourceLine" id="cb4-14" data-line-number="14">    inputs, targets,</a>
<a class="sourceLine" id="cb4-15" data-line-number="15">    num_iterations<span class="op">=</span><span class="dv">100</span>,</a>
<a class="sourceLine" id="cb4-16" data-line-number="16">    learning_rate<span class="op">=</span><span class="fl">0.01</span>,</a>
<a class="sourceLine" id="cb4-17" data-line-number="17">    batch_size<span class="op">=</span><span class="dv">50</span>)</a></code></pre></div>
<p>After training, you can access the trained model parameters as <code>neural_network.W_in</code> and <code>neural_network.W_out</code>. If you want to use them later, it is convenient to store them using <code>np.save(&quot;my_filename.npy&quot;, neural_network.W_in)</code>, and later load them using <code>neural_network.W_in = np.load(&quot;my_filename.npy&quot;)</code>.</p>
<div class="question">
<p>Use the code above to write a function that trains a feed forward neural network on next word prediction. It should take the hyperparameters <code>hidden_dim</code>, <code>learning_rate</code> and <code>batch_size</code> as arguments. Train models with three different hidden layer sizes and three different learning rates. Store the traces and afterwards plot the loss per iteration for all experiments. Can you explain what you find?</p>
</div>
<h1 id="word-embeddings"><span class="header-section-number">2</span> Word Embeddings</h1>
<p>So far, we have represented words as one hot vectors. Although computing those vectors is easy, the representation is far from ideal. It fails to capture any linguistic structure: words with similar meanings need not have similar vectors, for example. All one-hot vectors are equally far apart. But you might imagine that the better your word vectors capture linguistic properties (such as meaning or grammatical categories), the better you can solve linguistic tasks.</p>
<p>Now, it turns out that neural language models internally often <em>learn</em> better word representations, even if we don’t explicitly train them to do so. Take next word prediction: we update a network only based on its predictions, not on how it internally represents the words. But apparently the network can solve the task better if it also learns to represent the words in a linguistically more meaninguful way.</p>
<p>What is this magical internal representation? It is the vector of activations of the the first layer after the (one-hot) input layer. In complex networks with many layers, this layer is called the <em>embedding layer</em>: it embeds the one-hot vectors in some fixed-dimensional <em>embedding space</em>. In a well-trained model, this embedding space has typically picked up meaningful linguistic structure. This really is quite surprising – see <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations">this blogpost by Chris Olah</a> for some examples.</p>
<p>Let’s treat the hidden layer of our simple feed-forward network as an embedding layer and see if it reflects linguistic structure: do similar words cluster together? To do so, we compute the hidden state for every one-hot vector, and use the hidden states as embeddings. These are high-dimensional vectors, and to visualize them we use <span class="math inline">\(t\)</span>-SNE, <a href="https://distill.pub/2016/misread-tsne/">which is wonderfully illustrated in this article</a>.</p>
<div class="question">
<p>For each word in the training data, compute its word embedding: the corresponding hidden state. Feed the embeddings to the <span class="math inline">\(t\)</span>-SNE plotting function to plot the embeddings in a 2-dimensional space. Do you see any kind of regularity in this plot? Repeat this experiment with different sizes of hidden the layer (eg. 128, 256, 512):</p>
</div>
<h1 id="recurrent-neural-networks"><span class="header-section-number">3</span> Recurrent Neural Networks</h1>
<p>Feed forward neural networks deal with one input at a time: they have no way to deal with sequences of data, like text: sequences of words. We now turn to <em>recurrent neural networks</em> which were designed to deal with sequential data. They add a so called recurrent connection to the network, from hidden state to itself. As a result, at every timestep the hidden state depends not only on the input at that timestep, but also on the hidden state at the previous timestep. This history-dependency allows it to capture sequential structure. For some quick introductions to recurrent neural networks, you can look at <a href="https://medium.com/@camrongodbout/recurrent-neural-networks-for-beginners-7aca4e933b82#.aswkxfdhx">this blog post by Camron Godbout</a> or this <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">blog post by Denny Britz</a>.</p>
<p>Open <code>RNN.py</code>. Here you see the <code>RNN</code> class. This is an implementation of a recurrent neural network with one hidden layer. Take a look at the methods of the class. Read the comments if it’s not clear what each method is doing.</p>
<div class="question">
<p>Look into the <code>init_params</code> method, where the weight matrices, parameters, of the model are initialized. What is each weight matrix for? What are the dimensions of each of these matrices? What is the total number of parameters of this model?</p>
</div>
<div class="question">
<p>The <code>forward_pass</code> function is the method that defines how the output should be computed. Draw a graph that shows how the output is computed based on the input and the parameters.</p>
</div>
<div class="question">
<p>Initialize an <code>RNN</code> and apply it on the training data (before training, using randomly initialized parameters). What is the loss? Now initialize the input weight matrix with the learned embedding matrix from the feed forward network. What is the loss now? compare the results.</p>
</div>
<p>Since in recurrent neural networks, at each time step the output depends also on the hidden state at the previous step, the error also needs to be propagated to the previous time step. This is why it is called <strong>back propagation through time</strong> (BPTT).</p>
<div class="bonus">
<p>The <code>back_propagation_through_time</code> method is the implementation of the BPTT for a recurrent neural network with one hidden layer. Change the code so that it propagates the error up to <code>max_back_steps</code>. Train the RNN with the sentences datasets for different values for <code>max_back_steps</code>: 1, 3, 5 and 10. First, run the experiment for one iteration and plot the loss function per number of seen sentences. Then run the model for multiple iterations (until the loss function doesn’t decrease significantly) and plot the performance of the model per iteration.</p>
</div>
<div class="bonus">
<p>Use both <code>RNN</code> and <code>FeedForwardNN</code> to compute sentence prediction loss and generate sentences. Which one would you expect to perform better? Why? Are the results consistent with your executions? If not, why do you think this happens? Include performance plots and example sentences in your answer.</p>
<p><strong>Hint 1:</strong> To compute sentence loss for the feed-forward neural network use the <code>calculate_sentence_loss</code> method defined in the <code>FeedForwardNN</code> class.</p>
<p><strong>Hint 2:</strong> To generate sentences for both models use the <code>generate_sentence</code> method defined in each of the classes.</p>
</div>
    </div>
  </div>
</div>

</article>
<script src="http://projects.illc.uva.nl/LaCo/clclab/assets/bundle.js"></script>
</body>
</html>